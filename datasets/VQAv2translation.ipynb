{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ypNgSd4_jlI",
        "outputId": "151ca219-26ad-4b15-f288-8eaa42f4e537",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install datasets\n",
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "bEiXCzQkAVRq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VQAv2_train = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"train\").remove_columns(['question_type', 'answers', 'answer_type', 'question_id', 'image'])\n",
        "\n",
        "VQAv2_val = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"val\").remove_columns(['question_type', 'answers', 'answer_type', 'question_id', 'image'])"
      ],
      "metadata": {
        "id": "J9fGvjSxQoNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_img_format(img_id):\n",
        "  return f\"COCO_train2014_{img_id:012d}\"\n",
        "\n",
        "def val_img_format(img_id):\n",
        "  return f\"COCO_val2014_{img_id:012d}\""
      ],
      "metadata": {
        "id": "AZZWk6wc_pTP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VQAv2_train\n",
        "image_id_train = []\n",
        "answer_train = []\n",
        "question_train = []\n",
        "\n",
        "for feature in tqdm(VQAv2_train):\n",
        "    if len(feature[\"multiple_choice_answer\"]) <= 500 and len(feature[\"question\"]) <= 500:\n",
        "\n",
        "        answer_train.append(feature[\"multiple_choice_answer\"])\n",
        "        image = train_img_format(feature[\"image_id\"])\n",
        "        image_id_train.append(image)\n",
        "        question_train.append(feature[\"question\"])"
      ],
      "metadata": {
        "id": "74-lJhRfdMX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VQAv2_val\n",
        "image_id_val = []\n",
        "answer_val = []\n",
        "question_val = []\n",
        "\n",
        "for feature in tqdm(VQAv2_val):\n",
        "    if len(feature[\"multiple_choice_answer\"]) <= 500 and len(feature[\"question\"]) <= 500:\n",
        "\n",
        "        answer_val.append(feature[\"multiple_choice_answer\"])\n",
        "        image = val_img_format(feature[\"image_id\"])\n",
        "        image_id_val.append(image)\n",
        "        question_val.append(feature[\"question\"])"
      ],
      "metadata": {
        "id": "FLYSWV99P1XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def translation(model, tokenizer, sentences, batch_size):\n",
        "    sentences_ru = []\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "            batch = sentences[i: i + batch_size]\n",
        "\n",
        "            input_ids = tokenizer.batch_encode_plus(batch, padding=\"max_length\", max_length=512, return_tensors=\"pt\", truncation=True)[\"input_ids\"].to(device)\n",
        "            generated_tokens = model.generate(input_ids, max_length=512, forced_bos_token_id=tokenizer.lang_code_to_id[\"rus_Cyrl\"])\n",
        "            output_ids = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "            sentences_ru = sentences_ru + output_ids\n",
        "\n",
        "    return sentences_ru"
      ],
      "metadata": {
        "id": "ojVgKwvvdPb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RqywKGGydSGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "\n",
        "# VQAv2_train\n",
        "questions_train_ru = translation(model, tokenizer, question_train, batch_size)\n",
        "answers_train_ru = translation(model, tokenizer, answer_train, batch_size)"
      ],
      "metadata": {
        "id": "AN0qdaAvdUs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VQAv2_val\n",
        "questions_val_ru = translation(model, tokenizer, question_val, batch_size)\n",
        "answers_val_ru = translation(model, tokenizer, answer_val, batch_size)"
      ],
      "metadata": {
        "id": "0nbZVY3DQ-yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VQAv2_train\n",
        "questions_train = [i for i in questions_train_ru]\n",
        "answers_train = [i for i in answers_train_ru]\n",
        "max_len_train = len(question_train)"
      ],
      "metadata": {
        "id": "VcbqX6SidYLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VQAv2_val\n",
        "questions_val = [i for i in questions_val_ru]\n",
        "answers_val = [i for i in answers_val_ru]\n",
        "max_len_val = len(questions_val)"
      ],
      "metadata": {
        "id": "lfpj1L6IRQ-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_train = [{'image_id' : image_id_train[i], 'question' : questions_train[i], 'answer' : answers_train[i]} for i in range(max_len_train)]"
      ],
      "metadata": {
        "id": "anZq8qNxdajv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_val = [{'image_id' : image_id_val[i], 'question' : questions_val[i], 'answer' : answers_val[i]} for i in range(max_len_val)]"
      ],
      "metadata": {
        "id": "hQlbfMhbReeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "with jsonlines.open('VQAv2_train_translation.jsonl', mode='w') as writer:\n",
        "  writer.write(result_train)"
      ],
      "metadata": {
        "id": "jz243QjIdega"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open('VQAv2_val_translation.jsonl', mode='w') as writer:\n",
        "  writer.write(result_val)"
      ],
      "metadata": {
        "id": "ZAnWo-L0RyOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}