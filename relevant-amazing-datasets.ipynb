{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5060663,"sourceType":"datasetVersion","datasetId":2938312},{"sourceId":8185480,"sourceType":"datasetVersion","datasetId":4846819}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pycocotools\n!pip install open_clip_torch","metadata":{"_uuid":"3eda1577-101b-40ce-9227-cfb015359348","_cell_guid":"91929898-dac7-4068-bd56-2ec8fb030783","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T16:43:52.388508Z","iopub.execute_input":"2024-04-21T16:43:52.388967Z","iopub.status.idle":"2024-04-21T16:44:19.902829Z","shell.execute_reply.started":"2024-04-21T16:43:52.388932Z","shell.execute_reply":"2024-04-21T16:44:19.901469Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets import CocoDetection\n\nimport cv2\nfrom PIL import Image\n\nfrom transformers import AutoTokenizer\n# import open_clip\n\nfrom tqdm.auto import tqdm\nimport json\nimport sys","metadata":{"_uuid":"183d9723-81b4-4241-84c4-c1f11dc900ec","_cell_guid":"5d2a485b-ecc0-492c-996f-96aa4bcdcf3f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T16:44:19.905495Z","iopub.execute_input":"2024-04-21T16:44:19.905879Z","iopub.status.idle":"2024-04-21T16:44:25.844015Z","shell.execute_reply.started":"2024-04-21T16:44:19.905847Z","shell.execute_reply":"2024-04-21T16:44:25.842864Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pycocotools.coco import COCO","metadata":{"_uuid":"3c05789c-b5c2-461b-8f91-229af055a726","_cell_guid":"f1c53246-f43e-491d-8410-877f6c2d2933","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T16:44:25.845193Z","iopub.execute_input":"2024-04-21T16:44:25.846085Z","iopub.status.idle":"2024-04-21T16:44:25.860018Z","shell.execute_reply.started":"2024-04-21T16:44:25.846051Z","shell.execute_reply":"2024-04-21T16:44:25.858755Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# чекаем данные","metadata":{"_uuid":"96de673b-4265-4c20-a8b6-78a921658588","_cell_guid":"8f6cb29e-8990-4b8e-8ac9-0ebda4d54a8c","trusted":true}},{"cell_type":"code","source":"caption_path = '/kaggle/input/coco-captions-val/captions_val2014.json'\nwith open(caption_path, 'r') as f:\n    q = list(f)[0]\n    captions = json.loads(q)","metadata":{"_uuid":"04c54be8-f3ea-4fcf-8064-c954831ae6fe","_cell_guid":"7c58ea47-655b-4e4c-b7d0-cf32d4389180","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T14:33:43.304425Z","iopub.execute_input":"2024-04-21T14:33:43.304850Z","iopub.status.idle":"2024-04-21T14:33:44.028514Z","shell.execute_reply.started":"2024-04-21T14:33:43.304816Z","shell.execute_reply":"2024-04-21T14:33:44.027323Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"captions['images'][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-21T14:37:44.102452Z","iopub.execute_input":"2024-04-21T14:37:44.102870Z","iopub.status.idle":"2024-04-21T14:37:44.111057Z","shell.execute_reply.started":"2024-04-21T14:37:44.102833Z","shell.execute_reply":"2024-04-21T14:37:44.110022Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'license': 3,\n 'file_name': 'COCO_val2014_000000391895.jpg',\n 'coco_url': 'http://images.cocodataset.org/val2014/COCO_val2014_000000391895.jpg',\n 'height': 360,\n 'width': 640,\n 'date_captured': '2013-11-14 11:18:45',\n 'flickr_url': 'http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg',\n 'id': 391895}"},"metadata":{}}]},{"cell_type":"code","source":"for i, caption_s in tqdm(enumerate(captions)):\n    for j in range(len(captions[caption_s])):\n#         query = \"Что изображено на данной картинке?\"\n#         answer = captions[caption_s][j]\n        print(captions[caption_s]если A^3 = 0, значит обратная к A^2 это A, caption_s)\n#         self.img_paths.append(caption_s)\n#         self.query_tokens.append(torch.tensor(self.tokenizer.encode(query), dtype=torch.int64))\n#         self.answer_tokens.append(torch.tensor(self.tokenizer.encode(answer), dtype=torch.int64))\n#         max_seq_len = max(max_seq_len, self.answer_tokens[-1].shape[0])\n#     if i >= max_img:\n#         break\n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:08:46.285432Z","iopub.execute_input":"2024-04-21T15:08:46.285853Z","iopub.status.idle":"2024-04-21T15:08:46.306317Z","shell.execute_reply.started":"2024-04-21T15:08:46.285821Z","shell.execute_reply":"2024-04-21T15:08:46.305125Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d63527e09f4b8a8ca38836ddd8982c"}},"metadata":{}},{"name":"stdout","text":"{'description': 'COCO 2014 Dataset', 'url': 'http://cocodataset.org', 'version': '1.0', 'year': 2014, 'contributor': 'COCO Consortium', 'date_created': '2017/09/01'} info\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, caption in tqdm(enumerate(captions)):\n    for j in range(len(captions[caption])):\n#         query = \"Что изображено на картинке?\"\n        print(caption, caption[j])#, captions[caption][j])\n#         answer = captions_raw[caption\n#         self.img_paths += [caption]\n        #почему не сделать tokenizer.encode(query, padding=True, truncation=True) ?\n#         self.query_tokens += [torch.tensor(self.tokenizer.encode(query), dtype=torch.int64)]\n#         self.answer_tokens += [torch.tensor(self.tokenizer.encode(answer), dtype=torch.int64)]\n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:03:39.713147Z","iopub.execute_input":"2024-04-21T15:03:39.713836Z","iopub.status.idle":"2024-04-21T15:03:39.740941Z","shell.execute_reply.started":"2024-04-21T15:03:39.713787Z","shell.execute_reply":"2024-04-21T15:03:39.738709Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7547d4b27b0f4a5cb5b171bff5ad7f30"}},"metadata":{}},{"name":"stdout","text":"info i\n","output_type":"stream"}]},{"cell_type":"code","source":"class CocoDataset(Dataset):\n    def __init__(self, config, image_path, ann_file, caption_path, data_type=\"train\", coef_size=0.1,\n                 tokenizer_name=\"\", prefix_length=20, normalize_prefix=False):\n        if not tokenizer_name:\n            tokenizer_name = config.decoder\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        clip_model, _, self.preprocess = open_clip.create_model_and_transform(config.encoder, pretrained=\"laion400m_e32\")\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        self.img_path = image_path\n        self.ann_file = ann_file\n        self.caption_path = caption_path\n        \n        with open(caption_path, 'r') as f:\n            captions_raw = json.loads(list(f)[0])\n        \n        self.img_paths = []\n        self.query_tokens = []\n        self.answer_tokens = []\n        \n        \n        max_img = len(captions_raw)*coef_size \n        for i, caption in tqdm(enumerate(captions_raw), total=max_img):\n            for j in range(len(captions_raw[caption])):\n                query = \"Что изображено на картинке?\"\n                answer = captions_raw[caption][j]\n                self.img_paths += [caption]\n                #почему не сделать tokenizer.encode(query, padding=True, truncation=True) ?\n                self.query_tokens += [torch.tensor(self.tokenizer.encode(query), dtype=torch.int64)]\n                self.answer_tokens += [torch.tensor(self.tokenizer.encode(answer), dtype=torch.int64)]\n            if i >= max_img:\n                break\n        del captions_raw\n        sys.stdout.flush()\n        \n        #all_len\n        self.max_seq_len = prefix_length\n        self.type = data_type\n        \n    \"\"\"Почему не паддили captions?\"\"\"\n    def pad_tokens(self, item: int):\n        query_tokens = self.query_tokens[item]\n        padding = self.max_seq_len - query_tokens.shape[0]\n        if padding > 0:\n            query_tokens = torch.cat((query_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.query_tokens[item] = query_tokens\n        elif padding < 0:\n            query_tokens = query_tokens[:self.max_seq_len]\n            self.query_tokens[item] = query_tokens\n        query_mask = tokens.ge(0)  # mask is zero where we out of sequence\n        query_tokens[~query_mask] = 0\n        query_mask = query_mask.float()\n\n        answer_tokens = self.answer_tokens[item]\n        padding = self.max_seq_len - answer_tokens.shape[0]\n        if padding > 0:\n            answer_tokens = torch.cat((answer_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.answer_tokens[item] = answer_tokens\n        elif padding < 0:\n            answer_tokens = answer_tokens[:self.max_seq_len]\n            self.answer_tokens[item] = answer_tokens\n        answer_mask = tokens.ge(0)  # mask is zero where we out of sequence\n        answer_tokens[~answer_mask] = 0\n        answer_mask = answer_mask.float()\n        \n        return query_tokens, query_mask, answer_tokens, answer_mask\n    \n    def get_image(self, item):\n        name = str(self.img_paths[item])\n        name = f\"{self.img_path}/{name}\"\n        image_resized = cv2.resize(cv2.imread(naem), (256,256))\n        return Image.fromarray(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))\n\n    def __len__(self) -> int:\n        return len(self.img_paths)\n\n    def __getitem__(self, item):\n#         tokens, mask = self.pad_tokens(item)\n#         prefix = self.prefixes[item]\n#         if self.normalize_prefix:\n#             prefix = prefix.float()\n#             prefix = prefix / prefix.norm(2, -1)\n#         return tokens, mask, prefix\n        image = self.get_image(item)\n        image = self.preprocess(image).unsqueeze(0)\n        query_tokens, query_mask, answer_tokens, answer_mask = pad_tokens(item)\n        return query_tokens, query_mask, answer_tokens, answer_mask, image[0], item\n    def show_image(self, item):\n        image = self.get_image(item)\n        text = self.tokenizer.decode(self.pad_tokens(item)[2])\n        plt.imshow(img)\n        print(text)","metadata":{"_uuid":"2f52e834-278a-4438-8e59-c82acea6823a","_cell_guid":"d50b609f-0008-48b5-bf68-f2623473a33b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-19T19:53:43.286800Z","iopub.execute_input":"2024-04-19T19:53:43.287255Z","iopub.status.idle":"2024-04-19T19:53:43.317408Z","shell.execute_reply.started":"2024-04-19T19:53:43.287222Z","shell.execute_reply":"2024-04-19T19:53:43.316110Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"_uuid":"d0c76cf8-0b71-46ec-bcb5-f1f20f41db40","_cell_guid":"c9d64150-000a-4d29-8fe5-409ece44823f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T17:10:39.560163Z","iopub.execute_input":"2024-04-21T17:10:39.560704Z","iopub.status.idle":"2024-04-21T17:10:39.566117Z","shell.execute_reply.started":"2024-04-21T17:10:39.560644Z","shell.execute_reply":"2024-04-21T17:10:39.565135Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"vqa2 = load_dataset(\"HuggingFaceM4/VQAv2\")","metadata":{"_uuid":"858ed114-aba2-499a-8ad7-6bc3e6514984","_cell_guid":"e49909c1-5318-468f-904c-265f1e091f7e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T17:11:47.272937Z","iopub.execute_input":"2024-04-21T17:11:47.273555Z","iopub.status.idle":"2024-04-21T17:11:49.022236Z","shell.execute_reply.started":"2024-04-21T17:11:47.273507Z","shell.execute_reply":"2024-04-21T17:11:49.020942Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_vqa2 = vqa2[\"train\"]","metadata":{"_uuid":"a21df73c-a0c5-4c78-8036-445989da0dc7","_cell_guid":"76628360-ec3b-47bf-ac61-0bf416f26d07","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T17:16:08.042197Z","iopub.execute_input":"2024-04-21T17:16:08.042764Z","iopub.status.idle":"2024-04-21T17:16:08.049381Z","shell.execute_reply.started":"2024-04-21T17:16:08.042724Z","shell.execute_reply":"2024-04-21T17:16:08.048076Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# train_vqa2.set_format(\"pandas\")","metadata":{"_uuid":"56ccdc57-74e4-457e-9987-c59ae6d127f5","_cell_guid":"7dd229f4-4370-43b2-bbb1-30e7bdb577bc","collapsed":false,"execution":{"iopub.status.busy":"2024-04-19T21:29:59.524142Z","iopub.execute_input":"2024-04-19T21:29:59.524635Z","iopub.status.idle":"2024-04-19T21:29:59.530726Z","shell.execute_reply.started":"2024-04-19T21:29:59.524598Z","shell.execute_reply":"2024-04-19T21:29:59.529516Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instance = train_vqa2[1]","metadata":{"_uuid":"e5cdd673-f86e-4637-87a1-de1a7fb05f9d","_cell_guid":"a20a2e1b-d555-4c52-a8cc-ebf5fb8760fc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T17:16:13.944985Z","iopub.execute_input":"2024-04-21T17:16:13.945462Z","iopub.status.idle":"2024-04-21T17:16:14.003155Z","shell.execute_reply.started":"2024-04-21T17:16:13.945426Z","shell.execute_reply":"2024-04-21T17:16:14.000798Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_vqa2[\"question\"][:5]","metadata":{"execution":{"iopub.status.busy":"2024-04-21T14:32:49.634673Z","iopub.execute_input":"2024-04-21T14:32:49.635075Z","iopub.status.idle":"2024-04-21T14:32:50.120559Z","shell.execute_reply.started":"2024-04-21T14:32:49.635044Z","shell.execute_reply":"2024-04-21T14:32:50.119051Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['What is this photo taken looking through?',\n 'What position is this man playing?',\n 'What color is the players shirt?',\n 'Is this man a professional baseball player?',\n 'What color is the snow?']"},"metadata":{}}]},{"cell_type":"code","source":"instance","metadata":{"_uuid":"8beb92c3-447b-4772-a08f-76ab1a423575","_cell_guid":"bdf66cf9-212f-4e8e-9537-70293dec4cfc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-21T17:16:15.552453Z","iopub.execute_input":"2024-04-21T17:16:15.553035Z","iopub.status.idle":"2024-04-21T17:16:15.563042Z","shell.execute_reply.started":"2024-04-21T17:16:15.552998Z","shell.execute_reply":"2024-04-21T17:16:15.561683Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'question_type': 'what',\n 'multiple_choice_answer': 'pitcher',\n 'answers': [{'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 1},\n  {'answer': 'catcher', 'answer_confidence': 'no', 'answer_id': 2},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 3},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 4},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 5},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 6},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 7},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 8},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 9},\n  {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 10}],\n 'image_id': 458752,\n 'answer_type': 'other',\n 'question_id': 458752001,\n 'question': 'What position is this man playing?',\n 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}"},"metadata":{}}]},{"cell_type":"code","source":"class VQAv2Dataset(Dataset):\n    \"\"\"Датасет для файнтюна\"\"\"\n    def __init__(self, split, processor):\n        if (split == \"train\"):\n            self.dataset = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"train[:90%]\")\n        elif (split == \"val\"):\n            self.dataset = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"train[90%:]\")\n#         self.dataset = dataset\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        question = self.dataset[item]['question']\n        answer = self.dataset[item]['answer']\n        image_id = self.dataset[item]['pid']\n        image_path = f\"Data/train_fill_in_blank/{image_id}/image.png\"\n        image = Image.open(image_path).convert(\"RGB\")\n        text = question\n        \n        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        labels = self.processor.tokenizer.encode(\n            answer, max_length= 8, pad_to_max_length=True, return_tensors='pt'\n        )\n        encoding[\"labels\"] = labels\n\n        for k,v in encoding.items():  encoding[k] = v.squeeze()\n        return encoding","metadata":{"_uuid":"604b4a79-2d73-409a-ab6d-e624c915a7e6","_cell_guid":"14c55410-2285-429e-833f-999304e9f506","collapsed":false,"execution":{"iopub.status.busy":"2024-04-20T15:19:51.958357Z","iopub.execute_input":"2024-04-20T15:19:51.958761Z","iopub.status.idle":"2024-04-20T15:19:51.968694Z","shell.execute_reply.started":"2024-04-20T15:19:51.958707Z","shell.execute_reply":"2024-04-20T15:19:51.967809Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instance['image']","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:40:12.137976Z","iopub.execute_input":"2024-04-21T16:40:12.138548Z","iopub.status.idle":"2024-04-21T16:40:12.145297Z","shell.execute_reply.started":"2024-04-21T16:40:12.138512Z","shell.execute_reply":"2024-04-21T16:40:12.142414Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"answers = train_vqa2[\"multiple_choice_answer\"]\nquestions = train_vqa2[\"question\"]\n# images = train_vqa2[\"image\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:32:58.757589Z","iopub.execute_input":"2024-04-21T16:32:58.757984Z","iopub.status.idle":"2024-04-21T16:35:05.257010Z","shell.execute_reply.started":"2024-04-21T16:32:58.757956Z","shell.execute_reply":"2024-04-21T16:35:05.255228Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, answer in tqdm(enumerate(answers)):\n    print(i, answer, questions[i], answers[i])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:26:53.692390Z","iopub.execute_input":"2024-04-21T16:26:53.692871Z","iopub.status.idle":"2024-04-21T16:26:53.744734Z","shell.execute_reply.started":"2024-04-21T16:26:53.692835Z","shell.execute_reply":"2024-04-21T16:26:53.743886Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34aa4593db92482c876ffcee9073cc1b"}},"metadata":{}},{"name":"stdout","text":"0 net What is this photo taken looking through? net\n","output_type":"stream"}]},{"cell_type":"code","source":"class VQAv2_Dataset(Dataset):\n    def __init__(self, config, dataset, coef_size=0.1,\n                 tokenizer_name=\"\", prefix_length=20, normalize_prefix=False):\n        if not tokenizer_name:\n            tokenizer_name = config.decoder\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        clip_model, _, self.preprocess = open_clip.create_model_and_transform(config.encoder, pretrained=\"laion400m_e32\")\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        self.dataset = datatset  #load_dataset(\"HuggingFaceM4/VQAv2\", split=split)\n#         self.img_path = image_path\n#         self.ann_file = ann_file\n#         self.caption_path = caption_path\n#         coco_dataset_raw = CocoDetection(root=self.img_path, annFile=self.ann_file)\n        \n#         with open(caption_path, 'r') as f:\n#             captions_raw = json.loads(list(f)[0])\n        answers = dataset[\"multiple_choice_answer\"]\n        questions = dataset[\"question\"]\n        self.image_idx = []\n        self.query_tokens = []\n        self.answer_tokens = []\n        \n        max_img = len(answers)*coef_size \n        for i, answer in tqdm(enumerate(answers), total=max_img):\n            query = questions[i]\n            self.query_tokens += [torch.tensor(self.tokenizer.encode(query), dtype=torch.int64)]\n            self.answer_tokens += [torch.tensor(self.tokenizer.encode(answer), dtype=torch.int64)]\n            self.images += i\n            if i >= max_img:\n            break\n        del captions_raw\n        sys.stdout.flush()\n        \n        #all_len\n        self.max_seq_len = prefix_length\n        self.type = data_type\n        \n    \"\"\"Почему не паддили captions?\"\"\"\n    def pad_tokens(self, item: int):\n        query_tokens = self.query_tokens[item]\n        padding = self.max_seq_len - query_tokens.shape[0]\n        if padding > 0:\n            query_tokens = torch.cat((query_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.query_tokens[item] = query_tokens\n        elif padding < 0:\n            query_tokens = query_tokens[:self.max_seq_len]\n            self.query_tokens[item] = query_tokens\n        query_mask = tokens.ge(0)  # mask is zero where we out of sequence\n        query_tokens[~query_mask] = 0\n        query_mask = query_mask.float()\n\n\n        answer_tokens = self.answer_tokens[item]\n        padding = self.max_seq_len - answer_tokens.shape[0]\n        if padding > 0:\n            answer_tokens = torch.cat((answer_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.answer_tokens[item] = answer_tokens\n        elif padding < 0:\n            answer_tokens = answer_tokens[:self.max_seq_len]\n            self.answer_tokens[item] = answer_tokens\n        answer_mask = tokens.ge(0)  # mask is zero where we out of sequence\n        answer_tokens[~answer_mask] = 0\n        answer_mask = answer_mask.float()\n        \n        return query_tokens, query_mask, answer_tokens, answer_mask\n    \n    def get_image(self, item):\n#         name = str(self.img_paths[item])\n#         name = f\"{self.img_path}/{name}\"\n#         image_resized = cv2.resize(cv2.imread(naem), (256,256))\n#         return Image.fromarray(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))\n        image_resized = cv2.resize(self.image_idx[item], (256,256))\n        return Image.fromarray(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))\n\n    def __len__(self) -> int:\n        return len(self.image_idx)\n\n    def __getitem__(self, item):\n        image = self.get_image(item)\n        image = self.preprocess(image).unsqueeze(0)\n        query_tokens, query_mask, answer_tokens, answer_mask = pad_tokens(item)\n        return query_tokens, query_mask, answer_tokens, answer_mask, image[0], item\n    def show_image(self, item):\n        image = self.get_image(item)\n        text = self.tokenizer.decode(self.pad_tokens(item)[2])\n        plt.imshow(img)\n        print(text)","metadata":{"_uuid":"18448482-be7f-4a66-b6f1-e56fbefd05b3","_cell_guid":"a68a7d66-cd97-4bae-8b62-4cda247ab695","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}