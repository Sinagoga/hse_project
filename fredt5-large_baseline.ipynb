{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# %%capture\n","# %pip install open_clip_torch\n","# %pip install torchmetrics\n","# %pip install evaluate\n","# %pip install wandb\n","# %pip install pandas\n","# %pip install matplotlib\n","# %pip install numpy bert_score\n","# !pip install \"monai[einops]\"\n","# !pip install pip install rouge_score\n","# !pip install -U nltk "]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-13T23:14:37.534352Z","iopub.status.busy":"2024-05-13T23:14:37.533955Z","iopub.status.idle":"2024-05-13T23:15:01.119580Z","shell.execute_reply":"2024-05-13T23:15:01.118546Z","shell.execute_reply.started":"2024-05-13T23:14:37.534318Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as nnf\n","from torch.amp import autocast\n","from torch import einsum\n","import torch.nn.functional as F\n","\n","import open_clip\n","\n","from transformers import GPT2LMHeadModel, AutoTokenizer\n","from transformers import T5ForConditionalGeneration\n","from typing import Optional\n","\n","from transformers.optimization import Adafactor\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","import pickle\n","from torchmetrics.text import BLEUScore\n","from evaluate import load\n","from statistics import mean\n","import pandas as pd\n","# from vqadataset import VQAv2_Dataset\n","from einops import rearrange\n","import math\n","import wandb\n","from accelerate import Accelerator\n","from accelerate.utils import set_seed\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def exists(val):\n","    return val is not None\n","\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","\n","def stable_softmax(t, dim=-1):\n","    t = t - t.amax(dim=dim, keepdim=True)\n","    return t.softmax(dim=dim)\n","\n","\n","def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = nnf.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention\n","\n","\n","def expand_mask(mask):\n","    assert mask.ndim > 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n","    if mask.ndim == 3:\n","        mask = mask.unsqueeze(1)\n","    while mask.ndim < 4:\n","        mask = mask.unsqueeze(0)\n","    return mask\n","\n","\n","class BidirectionalCrossAttention(nn.Module):\n","    def __init__(\n","            self,\n","            *,\n","            dim,\n","            heads=8,\n","            dim_head=64,\n","            context_dim=None,\n","            dropout=0.,\n","            talking_heads=False,\n","            prenorm=False,\n","    ):\n","        super().__init__()\n","        context_dim = default(context_dim, dim)\n","        self.norm = nn.LayerNorm(dim) if prenorm else nn.Identity()\n","        self.context_norm = nn.LayerNorm(context_dim) if prenorm else nn.Identity()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        inner_dim = dim_head * heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.context_dropout = nn.Dropout(dropout)\n","        self.to_qk = nn.Linear(dim, inner_dim, bias=False)\n","        self.context_to_qk = nn.Linear(context_dim, inner_dim, bias=False)\n","        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n","        self.context_to_v = nn.Linear(context_dim, inner_dim, bias=False)\n","        self.to_out = nn.Linear(inner_dim, dim)\n","        self.context_to_out = nn.Linear(inner_dim, context_dim)\n","        self.talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else nn.Identity()\n","        self.context_talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else nn.Identity()\n","\n","    def forward(\n","            self,\n","            x,\n","            context,\n","            mask=None,\n","            context_mask=None,\n","            return_attn=False,\n","            rel_pos_bias=None\n","    ):\n","        b, i, j, h, device = x.shape[0], x.shape[-2], context.shape[-2], self.heads, x.device\n","        x = self.norm(x)\n","        context = self.context_norm(context)\n","        qk, v = self.to_qk(x), self.to_v(x)\n","        context_qk, context_v = self.context_to_qk(context), self.context_to_v(context)\n","        qk, context_qk, v, context_v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h),\n","                                           (qk, context_qk, v, context_v))\n","        sim = einsum('b h i d, b h j d -> b h i j', qk, context_qk) * self.scale\n","        if exists(rel_pos_bias):\n","            sim = sim + rel_pos_bias\n","        if exists(mask) or exists(context_mask):\n","            mask = default(mask, torch.ones((b, i), device=device, dtype=torch.bool))\n","            context_mask = default(context_mask, torch.ones((b, j), device=device, dtype=torch.bool))\n","            attn_mask = rearrange(mask, 'b i -> b 1 i 1') * rearrange(context_mask, 'b j -> b 1 1 j')\n","            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n","        attn = stable_softmax(sim, dim=-1)\n","        context_attn = stable_softmax(sim, dim=-2)\n","        attn = self.dropout(attn)\n","        context_attn = self.context_dropout(context_attn)\n","        attn = self.talking_heads(attn)\n","        context_attn = self.context_talking_heads(context_attn)\n","        out = einsum('b h i j, b h j d -> b h i d', attn, context_v)\n","        context_out = einsum('b h j i, b h j d -> b h i d', context_attn, v)\n","        out, context_out = map(lambda t: rearrange(t, 'b h n d -> b n (h d)'), (out, context_out))\n","        out = self.to_out(out)\n","        context_out = self.context_to_out(context_out)\n","        if return_attn:\n","            return out, context_out, attn, context_attn\n","        return out, context_out\n","\n","\n","class MultiheadAttention(nn.Module):\n","\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n","\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        nn.init.xavier_uniform_(self.qkv_proj.weight)\n","        self.qkv_proj.bias.data.fill_(0)\n","        nn.init.xavier_uniform_(self.o_proj.weight)\n","        self.o_proj.bias.data.fill_(0)\n","\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        if mask is not None:\n","            mask = expand_mask(mask)\n","        qkv = self.qkv_proj(x)\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3)\n","        q, k, v = qkv.chunk(3, dim=-1)\n","        values, attention = scaled_dot_product(q, k, v, mask=mask)\n","        values = values.permute(0, 2, 1, 3)\n","        values = values.reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o\n","\n","\n","class QFormerBlock(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, output_size, bias=True, act=nn.Tanh):\n","        super(QFormerBlock, self).__init__()\n","\n","        self.attn = MultiheadAttention(text_emb_size, text_emb_size, 16)\n","        self.cross_attn = BidirectionalCrossAttention(\n","            dim=img_emb_size,\n","            heads=16,\n","            dim_head=1024,\n","            context_dim=text_emb_size\n","        )\n","        self.text_mlp = nn.Sequential(\n","            nn.Linear(text_emb_size, text_emb_size * 2),\n","            act(),\n","            nn.Linear(text_emb_size * 2, text_emb_size * 2),\n","            act(),\n","            nn.Linear(text_emb_size * 2, output_size)\n","        )\n","\n","    @autocast(\"cuda\")\n","    def forward(self, img_emb: torch.Tensor, text_emb: torch.Tensor) -> torch.Tensor:\n","        text_emb = self.attn(text_emb)\n","        img_emb, text_emb = self.cross_attn(img_emb.reshape(-1, 1, img_emb.shape[1]), text_emb)\n","        text_emb = self.text_mlp(text_emb)\n","        return img_emb, text_emb\n","\n","\n","class MySequential(nn.Sequential):\n","    def forward(self, *inp):\n","        for module in self._modules.values():\n","            inp = module(*inp)\n","            if inp[0].shape[1] == 1:\n","                inp = (inp[0][:, 0, :], inp[1])\n","        return inp\n","\n","\n","class QFormer(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, output_size, n_blocks=4, bias=True, act=nn.Tanh):\n","        super(QFormer, self).__init__()\n","\n","        self.blocks = MySequential(\n","            *[QFormerBlock(img_emb_size, text_emb_size, text_emb_size) for _ in range(n_blocks)],\n","        )\n","        self.res = nn.Sequential(\n","            nn.Linear(img_emb_size + text_emb_size, output_size)\n","        )\n","\n","    @autocast(\"cuda\")\n","    def forward(self, img_emb: torch.Tensor, text_emb: torch.Tensor) -> torch.Tensor:\n","        img_emb, text_emb = self.blocks(img_emb, text_emb)\n","        text_emb = text_emb.mean(axis=1)\n","        res_emb = torch.cat((img_emb, text_emb), axis=1)\n","        res_emb = self.res(res_emb)\n","        return res_emb\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_shape, output_shape, act=nn.Tanh):\n","        super(MLP, self).__init__()\n","        self.seq = nn.Sequential(\n","            nn.Linear(input_shape, input_shape * 2),\n","            act(),\n","            nn.Linear(input_shape * 2, output_shape)\n","        )\n","\n","    @autocast(\"cuda\")\n","    def forward(self, x):\n","        return self.seq(x)\n","\n","\n","def freeze(\n","        model,\n","        freeze_emb=False,\n","        freeze_ln=False,\n","        freeze_attn=False,\n","        freeze_ff=False,\n","        freeze_other=False,\n","):\n","    for name, p in model.named_parameters():\n","        name = name.lower()\n","        if 'ln' in name or 'norm' in name:\n","            p.requires_grad = not freeze_ln\n","        elif 'embeddings' in name:\n","            p.requires_grad = not freeze_emb\n","        elif 'mlp' in name:\n","            p.requires_grad = not freeze_ff\n","        elif 'attn' in name:\n","            p.requires_grad = not freeze_attn\n","        else:\n","            p.requires_grad = not freeze_other\n","\n","    return model\n","\n","\n","class ClipCaptionModel(nn.Module):\n","    def __init__(self, config, prefix_length: int, prefix_size: int = 640, dist_loss=nn.MSELoss()):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","        self.gpt = T5ForConditionalGeneration.from_pretrained(config.decoder,\n","                                                   eos_token_id=self.tokenizer.pad_token_id)\n","        self.gpt_embedding_size = self.gpt.get_input_embeddings().weight.shape[1]\n","        self.clip_project = QFormer(prefix_size, self.gpt_embedding_size,\n","                                    self.gpt_embedding_size * prefix_length)\n","        self.device = config.device\n","        self.dist_loss = dist_loss\n","        self.mlp = MLP(self.gpt_embedding_size, self.gpt_embedding_size)\n","\n","        for p in self.gpt.parameters():\n","            p.requires_grad = False\n","        for p in self.clip_model.parameters():\n","            p.requires_grad = False\n","\n","    def get_text_embeddings(self, tokens):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        return embedding_text\n","    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    @autocast(\"cuda\")\n","    def forward(self, query_tokens: torch.Tensor, query_mask: Optional[torch.Tensor],\n","                answer_tokens: torch.Tensor, answer_mask: Optional[torch.Tensor], image):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt(inputs_embeds=prefix_projections, labels=answer_tokens)\n","        return out, prefix_projections\n","\n","    def generate(self, image, texts, max_seq_len):\n","        tokens = torch.tensor(self.tokenizer.batch_encode_plus(texts, padding='max_length', max_length=max_seq_len, truncation=True)['input_ids'], dtype=torch.int64).to(self.device)\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt.generate(\n","            inputs_embeds=prefix_projections,\n","            max_new_tokens=self.prefix_length,\n","            no_repeat_ngram_size=3,\n","            repetition_penalty=2.,\n","        )\n","        res = [decode_question(x, self.tokenizer) for x in out]\n","        return res"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","import sys\n","from matplotlib import pyplot as plt\n","import json\n","from PIL import Image\n","class VQAv2_Dataset(Dataset):\n","    def __init__(self, config, dataset_path, coef_size=0.1,\n","                 tokenizer_name=\"\", prefix_length=20, normalize_prefix=False, imagespath_split=None):\n","        if not tokenizer_name:\n","            tokenizer_name = config.decoder\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        clip_model, _, self.preprocess = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.prefix_length = prefix_length\n","        self.normalize_prefix = normalize_prefix\n","\n","        with open(dataset_path, 'r') as f:\n","            dataset = json.loads(list(f)[0])\n","\n","        self.img_paths = []\n","        self.query_tokens = []\n","        self.answer_tokens = []\n","\n","        max_img = len(dataset)*coef_size\n","        for i, el in tqdm(enumerate(dataset), total=max_img):\n","            answer = el['answer'] \n","            question = el['question']\n","            # self.query_tokens += [torch.tensor(self.tokenizer.encode(question), dtype=torch.int64)]\n","            # self.answer_tokens += [torch.tensor(self.tokenizer.encode(answer), dtype=torch.int64)]\n","            self.query_tokens += [self.tokenizer.encode(question, return_tensors=\"pt\",padding='max_length', max_length=prefix_length, truncation=True)]\n","            self.answer_tokens += [self.tokenizer.encode(answer, return_tensors=\"pt\", padding='max_length', max_length=prefix_length, truncation=True)]\n","            if (\"val\" in imagespath_split):\n","                self.img_paths += [imagespath_split + el['image_id'].replace(\"train\", \"val\") + \".jpg\"]\n","            else:\n","                self.img_paths += [imagespath_split + el['image_id'] + \".jpg\"]\n","            if int(i) >= max_img:\n","                  break\n","        del dataset\n","        sys.stdout.flush()\n","\n","        #all_len\n","        self.max_seq_len = prefix_length\n","        # self.type = data_type\n","\n","    \"\"\"Почему не паддили captions?\"\"\"\n","    def pad_tokens(self, item: int):\n","        query_tokens = self.query_tokens[item]\n","        # padding = self.max_seq_len - query_tokens.shape[0]\n","        # if padding > 0:\n","        #     query_tokens = torch.cat((query_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n","        #     self.query_tokens[item] = query_tokens\n","        # elif padding < 0:\n","        #     query_tokens = query_tokens[:self.max_seq_len]\n","        #     self.query_tokens[item] = query_tokens\n","        # query_mask = query_tokens.ge(0)  # mask is zero where we out of sequence\n","        # query_tokens[~query_mask] = 0\n","        # query_mask = query_mask.float()\n","\n","\n","        answer_tokens = self.answer_tokens[item]\n","        # padding = self.max_seq_len - answer_tokens.shape[0]\n","        # if padding > 0:\n","        #     answer_tokens = torch.cat((answer_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n","        #     self.answer_tokens[item] = answer_tokens\n","        # elif padding < 0:\n","        #     answer_tokens = answer_tokens[:self.max_seq_len]\n","        #     self.answer_tokens[item] = answer_tokens\n","        # answer_mask = answer_tokens.ge(0)  # mask is zero where we out of sequence\n","        # answer_tokens[~answer_mask] = 0\n","        # answer_mask = answer_mask.float()\n","        query_mask = query_tokens\n","        answer_mask = answer_tokens\n","        return query_tokens[0], query_mask[0], answer_tokens[0], answer_mask[0]\n","\n","    def get_image(self, item):\n","        name = str(self.img_paths[item])\n","        # name = f\"{self.img_path}/{name}\"\n","        image_resized = Image.open(name)\n","        image_resized = image_resized.resize((256, 256))\n","        return image_resized\n","        # image_resized = cv2.resize(self.image_idx[item], (256,256))\n","        # return Image.fromarray(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))\n","\n","    def __len__(self) -> int:\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, item):\n","        image = self.get_image(item)\n","        image = self.preprocess(image).unsqueeze(0)\n","        query_tokens, query_mask, answer_tokens, answer_mask = self.pad_tokens(item)\n","        return query_tokens, query_mask, answer_tokens, answer_mask, image[0], item\n","        # return query_tokens, query_mask, answer_tokens, answer_mask, item\n","\n","    def show_image(self, item):\n","        image = self.get_image(item)\n","        text = self.tokenizer.decode(self.pad_tokens(item)[2])\n","        plt.imshow(image)\n","        print(text)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class Config:\n","    encoder: str = \"ViT-B-16-plus-240\"\n","    decoder: str = \"ai-forever/FRED-T5-large\"\n","    batch_size: int = 128\n","    num_epochs: int = 40\n","    frozen_gpt: int = 8\n","    frozen_clip: int = 24\n","    learning_rate: float  = 2e-4\n","    save_path: str = \"/home/jovyan/vqa_project/baselines/saved_models_FRED-T5-large/fulliliya/\"\n","    prefix_length: int = 20\n","    only_prefix: int = False\n","    prefix: str = \"prefix_small\"\n","    device: str = \"cuda:1\"\n","    save_every: int = 1\n","    warmup_steps: int = 2000"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:10.238419Z","iopub.status.busy":"2024-05-12T16:35:10.237459Z","iopub.status.idle":"2024-05-12T16:35:12.132381Z","shell.execute_reply":"2024-05-12T16:35:12.131436Z","shell.execute_reply.started":"2024-05-12T16:35:10.238377Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["bertscore = load(\"bertscore\")\n","meteor = load('meteor')\n","rouge = load('rouge')\n","bleu_scorers = [BLEUScore(n_gram=i) for i in [1, 2, 3]] + [bertscore, meteor, rouge]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:14.372918Z","iopub.status.busy":"2024-05-12T16:35:14.372562Z","iopub.status.idle":"2024-05-12T16:35:46.471596Z","shell.execute_reply":"2024-05-12T16:35:46.470408Z","shell.execute_reply.started":"2024-05-12T16:35:14.372887Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrajanmihajlov\u001b[0m (\u001b[33maid_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/jovyan/vqa_project/baselines/wandb/run-20240523_043223-3typx4dt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/aid_/baseline-vqa-rugpt/runs/3typx4dt' target=\"_blank\">sleek-darkness-24</a></strong> to <a href='https://wandb.ai/aid_/baseline-vqa-rugpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/aid_/baseline-vqa-rugpt' target=\"_blank\">https://wandb.ai/aid_/baseline-vqa-rugpt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/aid_/baseline-vqa-rugpt/runs/3typx4dt' target=\"_blank\">https://wandb.ai/aid_/baseline-vqa-rugpt/runs/3typx4dt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aid_/baseline-vqa-rugpt/runs/3typx4dt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fe4e160f3a0>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"278590c2621521efe866317352d7f3e13fef885f\")\n","wandb.init(project=\"baseline-vqa-rugpt\", sync_tensorboard=True, name=\"\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:17:48.923946Z","iopub.status.busy":"2024-05-13T23:17:48.923232Z","iopub.status.idle":"2024-05-13T23:17:48.929456Z","shell.execute_reply":"2024-05-13T23:17:48.928365Z","shell.execute_reply.started":"2024-05-13T23:17:48.923911Z"},"trusted":true},"outputs":[],"source":["def decode_question(question_token, tokenizer):\n","    decoded_string = tokenizer.decode(question_token)\n","    # if \"<pad>\" in decoded_string:\n","    #     truncate_pads = decoded_string.index(\"<pad>\")\n","    #     decoded_string = decoded_string[:truncate_pads]\n","    decoded_string = decoded_string.replace(\"<pad>\", \"\")\n","    return decoded_string"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:17:52.010474Z","iopub.status.busy":"2024-05-13T23:17:52.009843Z","iopub.status.idle":"2024-05-13T23:17:52.021926Z","shell.execute_reply":"2024-05-13T23:17:52.020878Z","shell.execute_reply.started":"2024-05-13T23:17:52.010442Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def train(model, optimizer, scheduler, loss_func, loader, epoch, args):\n","    model.train()\n","    pbar = tqdm(loader, total=len(loader))\n","    step = 0\n","    for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","\n","        query_tokens, query_mask, prefix = query_tokens.to(args.device), query_mask.to(args.device), prefix.to(\n","            args.device, dtype=torch.bfloat16)\n","        answer_tokens, answer_mask = answer_tokens.to(args.device), answer_mask.to(args.device)\n","        # print(query_tokens.size(), query_mask.size(), answer_tokens.size(), answer_mask.size(), prefix.size())\n","        outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","        logits = outputs.logits\n","        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), answer_tokens.flatten().to(torch.int64),\n","                                 ignore_index=0)\n","\n","        loss2 = model.dist_loss(model.get_text_embeddings(answer_tokens).to(torch.float32), proj.to(torch.float32))\n","        loss += loss2\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n","\n","        #backpropogation\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        pbar.set_postfix({\"loss\": loss.item(), \"dist_loss\": loss2.item()})\n","        wandb.log({\"loss\": loss.item(), \"dist_loss\": loss2.item()})\n","        step += 1\n","        if step % 1000 == 0:\n","            print(\"QUESTION:\", decode_question(query_tokens[0], train_dataset.tokenizer))\n","            print(\"ANSWER:\", decode_question(answer_tokens[0], train_dataset.tokenizer))\n","            print(\"PREDICTED: \", model.generate(torch.tensor([train_dataset[idx[0]][4].tolist()]).to(args.device),\n","                                                [decode_question(query_tokens[0], model.tokenizer)], train_dataset.max_seq_len)[0])\n","    with open(f'{args.save_path}checkpoint_{epoch}.pkl', 'wb') as f:\n","        pickle.dump(model, f)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:46.499711Z","iopub.status.busy":"2024-05-12T16:35:46.499126Z","iopub.status.idle":"2024-05-12T16:35:46.521515Z","shell.execute_reply":"2024-05-12T16:35:46.520544Z","shell.execute_reply.started":"2024-05-12T16:35:46.499679Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, optimizer, scheduler, loss_func, loader, args):\n","    model.eval()\n","    pbar = tqdm(loader, total=len(loader))\n","    step = 0\n","\n","    bl1 = []\n","    bl2 = []\n","    bl3 = []\n","    brt = []\n","    mtr = []\n","    rg = []\n","    val_losses = []\n","    val_dist = []\n","    for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","        query_tokens, query_mask, prefix = query_tokens.to(args.device), query_mask.to(args.device), prefix.to(\n","            args.device, dtype=torch.bfloat16)\n","        answer_tokens, answer_mask = answer_tokens.to(args.device), answer_mask.to(args.device)\n","        outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","        logits = outputs.logits\n","        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), answer_tokens.flatten().to(torch.int64),\n","                                 ignore_index=0)\n","        loss2 = model.dist_loss(model.get_text_embeddings(answer_tokens), proj)\n","\n","        # real = model.tokenizer.batch_decode(answer_tokens)\n","        real = [decode_question(answer_tokens[i], model.tokenizer) for i in range(len(answer_tokens))]\n","#         pred = model.generate(torch.tensor([val_dataset[idx[j]][4].tolist() for j in range(len(idx))]).to(args.device),\n","#                               [\"Что на картинке?\" for _ in range(len(idx))])\n","        pred = model.generate(torch.tensor([val_dataset[idx[j]][4].tolist() for j in range(len(idx))]).to(args.device),\n","                              [decode_question(query_tokens[j], model.tokenizer) for j in range(len(idx))], val_dataset.max_seq_len)\n","        \n","#         model.generate(torch.tensor([train_dataset[idx[0]][4].tolist()]).to(args.device),\n","#                                                 decode_question(query_tokens, model.tokenizer))[0]\n","        \n","        # real = truncate_sentences(real)\n","        # pred = truncate_sentences(pred)\n","        \n","        bl1.append(bleu_scorers[0](pred, real))\n","        bl2.append(bleu_scorers[1](pred, real))\n","        bl3.append(bleu_scorers[2](pred, real))\n","        brt.append(bleu_scorers[3].compute(predictions=pred, references=real, lang=\"ru\")['f1'])\n","        mtr.append(bleu_scorers[4].compute(predictions=pred, references=real)['meteor'])\n","        rg.append(bleu_scorers[5].compute(predictions=pred, references=real)['rougeL'])\n","\n","        if step % 400 == 0:\n","            print(\"QUESTION:\", decode_question(query_tokens[0], val_dataset.tokenizer))\n","            print(\"TEXT:\", real[0])\n","            print(\"PREDICTED: \", pred[0])\n","\n","            imgs = []\n","            for j in range(len(idx)):\n","                wa_img = wandb.Image(\n","                    val_dataset.get_image(idx[j]),\n","                    caption=f\"REAL : {real[j]}, PREDICTED : {pred[j]}\"\n","                )\n","                imgs.append(wa_img)\n","\n","            wandb.log({\"Generations.\": imgs})\n","\n","        step += 1\n","\n","        pbar.set_postfix({\"val_loss\": loss.item(), \"val_dist\": loss2.item()})\n","        val_losses.append(loss.item())\n","        val_dist.append(loss2.item())\n","\n","    wandb.log({\"val_loss\": mean(val_losses),\n","               \"val_dist\": mean(val_dist)})\n","\n","    wandb.log({\n","        \"bleu_1\": mean([tensor.item() for tensor in bl1]),\n","        \"bleu_2\": mean([tensor.item() for tensor in bl2]),\n","        \"bleu_3\": mean([tensor.item() for tensor in bl3]),\n","        \"bert_score\": np.mean([tensor for tensor in brt]),\n","        \"meteor_score\": np.mean([tensor for tensor in mtr]),\n","        \"rouge_score\": np.mean([tensor for tensor in rg])\n","    })\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:36:04.797205Z","iopub.status.busy":"2024-05-12T16:36:04.796511Z","iopub.status.idle":"2024-05-12T16:36:04.809456Z","shell.execute_reply":"2024-05-12T16:36:04.808123Z","shell.execute_reply.started":"2024-05-12T16:36:04.797173Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def fit_model(args, model, train_loader, val_loader):\n","    wandb.config = {\n","        \"learning_rate\": args.learning_rate,\n","        \"epochs\": args.num_epochs,\n","        \"batch_size\": args.batch_size\n","    }\n","\n","    # if not os.path.exists(args.save_path):\n","    #     os.makedirs(args.save_path)\n","    device = args.device\n","\n","    # model = ClipCaptionModel(args, args.prefix_length)\n","    model = model.to(args.device)\n","\n","    wandb.watch(model, log_freq=10, log=\"gradients\")\n","\n","    model.train()\n","\n","    loss_func = nn.CrossEntropyLoss()\n","    optimizer = Adafactor(model.parameters(), lr=args.learning_rate,\n","                          relative_step=False  # for adafactor\n","                          )\n","\n","    # train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","    # val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=15000\n","    )\n","    # evaluate(model, optimizer, scheduler, loss_func, val_loader, args)\n","    print(\"Start train model\")\n","    for epoch in range(args.num_epochs):\n","        if epoch == args.frozen_gpt:\n","            print(\"GPT UNFROZEN\")\n","            for p in model.gpt.parameters():\n","                p.requires_grad = True\n","        if epoch == args.frozen_clip:\n","            print(\"CLIP UNFROZEN\")\n","            for p in model.clip_model.parameters():\n","                p.requires_grad = True\n","        print(f\"---------- Train epoch {epoch} ---------\")\n","        train(model, optimizer, scheduler, loss_func, train_loader, epoch, args)\n","        print(f\"---------- Evaluate epoch {epoch} ---------\")\n","        evaluate(model, optimizer, scheduler, loss_func, val_loader, args)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:18:13.833872Z","iopub.status.busy":"2024-05-13T23:18:13.833039Z","iopub.status.idle":"2024-05-13T23:18:22.979012Z","shell.execute_reply":"2024-05-13T23:18:22.978052Z","shell.execute_reply.started":"2024-05-13T23:18:13.833841Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","221879it [00:41, 5284.97it/s]                              \n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"," 97%|█████████▋| 10377/10717.7 [00:02<00:00, 4252.30it/s]/home/user/conda/lib/python3.9/site-packages/tqdm/std.py:639: TqdmWarning: clamping frac to range [0, 1]\n","  full_bar = Bar(frac,\n","100%|██████████| 10718/10717.7 [00:02<00:00, 4213.08it/s]\n"]}],"source":["config = Config()\n","train_dataset = VQAv2_Dataset(config, dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_train_translation.jsonl\", imagespath_split=\"/home/jovyan/vqa_project/baselines/trainvqa/train2014/\", coef_size=0.5)\n","val_dataset = VQAv2_Dataset(config, dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_val_translation.jsonl\", imagespath_split=\"/home/jovyan/vqa_project/baselines/valvqa/val2014/\", coef_size=0.05)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:53:37.689100Z","iopub.status.busy":"2024-05-13T23:53:37.688719Z","iopub.status.idle":"2024-05-13T23:53:47.886538Z","shell.execute_reply":"2024-05-13T23:53:47.885546Z","shell.execute_reply.started":"2024-05-13T23:53:37.689075Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["model = ClipCaptionModel(config, config.prefix_length)\n","# model = pickle.load(open(\"/home/jovyan/vqa_project/baselines/saved_models/full/checkpoint_2.pkl\", 'rb'))\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start train model\n","---------- Train epoch 0 ---------\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 999/1734 [18:40<09:27,  1.29it/s, loss=nan, dist_loss=nan]  "]},{"name":"stdout","output_type":"stream","text":["QUESTION: Что написано на сноубордах?\n","ANSWER: Бёртон\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 1000/1734 [18:41<31:57,  2.61s/it, loss=nan, dist_loss=nan]"]},{"name":"stdout","output_type":"stream","text":["PREDICTED:  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1734/1734 [32:32<00:00,  1.13s/it, loss=nan, dist_loss=nan]\n"]},{"name":"stdout","output_type":"stream","text":["---------- Evaluate epoch 0 ---------\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/84 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"]},{"name":"stdout","output_type":"stream","text":["QUESTION: - Сколько времени? - Пять часов.\n","TEXT: 12:05\n","PREDICTED:  \n"]},{"name":"stderr","output_type":"stream","text":["WARNING:root:Only 108 Image will be uploaded.\n","  1%|          | 1/84 [01:08<1:34:04, 68.00s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","  2%|▏         | 2/84 [01:22<49:58, 36.57s/it, val_loss=nan, val_dist=nan]  Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","  4%|▎         | 3/84 [01:38<36:21, 26.93s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","  5%|▍         | 4/84 [01:56<31:25, 23.57s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","  6%|▌         | 5/84 [02:11<27:07, 20.60s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","  7%|▋         | 6/84 [02:26<24:02, 18.50s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","  8%|▊         | 7/84 [02:45<23:59, 18.69s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 10%|▉         | 8/84 [03:01<22:33, 17.80s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 11%|█         | 9/84 [03:15<20:41, 16.56s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 12%|█▏        | 10/84 [03:33<21:07, 17.13s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 13%|█▎        | 11/84 [03:50<20:44, 17.04s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 14%|█▍        | 12/84 [04:03<19:12, 16.01s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 15%|█▌        | 13/84 [04:18<18:30, 15.64s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 17%|█▋        | 14/84 [04:37<19:14, 16.49s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 18%|█▊        | 15/84 [04:51<18:04, 15.72s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 19%|█▉        | 16/84 [05:04<17:10, 15.15s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 20%|██        | 17/84 [05:24<18:20, 16.43s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 21%|██▏       | 18/84 [05:39<17:33, 15.97s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 23%|██▎       | 19/84 [05:56<17:35, 16.23s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 24%|██▍       | 20/84 [06:12<17:29, 16.40s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 25%|██▌       | 21/84 [06:30<17:40, 16.84s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 26%|██▌       | 22/84 [06:48<17:40, 17.10s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 27%|██▋       | 23/84 [07:06<17:36, 17.32s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 29%|██▊       | 24/84 [07:23<17:26, 17.44s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 30%|██▉       | 25/84 [07:41<17:16, 17.56s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 31%|███       | 26/84 [07:59<16:56, 17.53s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 32%|███▏      | 27/84 [08:17<16:46, 17.65s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 33%|███▎      | 28/84 [08:34<16:18, 17.48s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 35%|███▍      | 29/84 [08:52<16:08, 17.62s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 36%|███▌      | 30/84 [09:10<16:04, 17.87s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 37%|███▋      | 31/84 [09:27<15:31, 17.58s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 38%|███▊      | 32/84 [09:45<15:20, 17.70s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 39%|███▉      | 33/84 [10:03<15:05, 17.76s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 40%|████      | 34/84 [10:21<14:44, 17.69s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 42%|████▏     | 35/84 [10:37<14:15, 17.47s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 43%|████▎     | 36/84 [10:55<13:55, 17.41s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 44%|████▍     | 37/84 [11:13<13:54, 17.76s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 45%|████▌     | 38/84 [11:30<13:24, 17.50s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 46%|████▋     | 39/84 [11:48<13:10, 17.57s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 48%|████▊     | 40/84 [12:06<12:57, 17.66s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 49%|████▉     | 41/84 [12:23<12:35, 17.56s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 50%|█████     | 42/84 [12:39<12:02, 17.20s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 51%|█████     | 43/84 [12:58<11:58, 17.52s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 52%|█████▏    | 44/84 [13:15<11:34, 17.37s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 54%|█████▎    | 45/84 [13:32<11:11, 17.21s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 55%|█████▍    | 46/84 [13:50<11:12, 17.70s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 56%|█████▌    | 47/84 [14:07<10:39, 17.27s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 57%|█████▋    | 48/84 [14:23<10:11, 16.97s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 58%|█████▊    | 49/84 [14:41<10:09, 17.40s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 60%|█████▉    | 50/84 [14:58<09:45, 17.22s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 61%|██████    | 51/84 [15:16<09:29, 17.26s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 62%|██████▏   | 52/84 [15:34<09:22, 17.58s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 63%|██████▎   | 53/84 [15:51<09:05, 17.59s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 64%|██████▍   | 54/84 [16:08<08:37, 17.26s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 65%|██████▌   | 55/84 [16:25<08:21, 17.28s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 67%|██████▋   | 56/84 [16:44<08:12, 17.57s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 68%|██████▊   | 57/84 [17:01<07:50, 17.42s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 69%|██████▉   | 58/84 [17:18<07:30, 17.32s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 70%|███████   | 59/84 [17:36<07:16, 17.48s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 71%|███████▏  | 60/84 [17:53<06:58, 17.46s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 73%|███████▎  | 61/84 [18:11<06:45, 17.65s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 74%|███████▍  | 62/84 [18:28<06:21, 17.35s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 75%|███████▌  | 63/84 [18:41<05:40, 16.20s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 76%|███████▌  | 64/84 [18:57<05:18, 15.93s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 77%|███████▋  | 65/84 [19:23<06:01, 19.04s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 79%|███████▊  | 66/84 [19:34<04:58, 16.59s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 80%|███████▉  | 67/84 [19:45<04:15, 15.03s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 81%|████████  | 68/84 [19:57<03:44, 14.01s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 82%|████████▏ | 69/84 [20:14<03:44, 14.98s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 83%|████████▎ | 70/84 [20:26<03:15, 13.94s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 85%|████████▍ | 71/84 [20:42<03:12, 14.84s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 86%|████████▌ | 72/84 [21:01<03:10, 15.86s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 87%|████████▋ | 73/84 [21:14<02:47, 15.19s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 88%|████████▊ | 74/84 [21:28<02:27, 14.72s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 89%|████████▉ | 75/84 [21:46<02:22, 15.80s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 90%|█████████ | 76/84 [22:01<02:03, 15.39s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 92%|█████████▏| 77/84 [22:14<01:42, 14.70s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 93%|█████████▎| 78/84 [22:34<01:37, 16.30s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 94%|█████████▍| 79/84 [22:52<01:23, 16.73s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 95%|█████████▌| 80/84 [23:05<01:03, 15.88s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 96%|█████████▋| 81/84 [23:20<00:46, 15.39s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 98%|█████████▊| 82/84 [23:38<00:32, 16.22s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"," 99%|█████████▉| 83/84 [23:55<00:16, 16.62s/it, val_loss=nan, val_dist=nan]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","100%|██████████| 84/84 [24:04<00:00, 17.20s/it, val_loss=nan, val_dist=nan]\n"]},{"ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (84,) + inhomogeneous part.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fit_model(config, model, train_loader, val_loader)\n","\u001b[1;32m/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m train(model, optimizer, scheduler, loss_func, train_loader, epoch, args)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m---------- Evaluate epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m ---------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m evaluate(model, optimizer, scheduler, loss_func, val_loader, args)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;32m/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     val_dist\u001b[39m.\u001b[39mappend(loss2\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m: mean(val_losses),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mval_dist\u001b[39m\u001b[39m\"\u001b[39m: mean(val_dist)})\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbleu_1\u001b[39m\u001b[39m\"\u001b[39m: mean([tensor\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m bl1]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbleu_2\u001b[39m\u001b[39m\"\u001b[39m: mean([tensor\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m bl2]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbleu_3\u001b[39m\u001b[39m\"\u001b[39m: mean([tensor\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m bl3]),\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbert_score\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39;49mmean([tensor \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m brt]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmeteor_score\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmean([tensor \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m mtr]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrouge_score\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmean([tensor \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m rg])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m })\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3505\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/numpy/core/_methods.py:102\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 102\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    104\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (84,) + inhomogeneous part."]}],"source":["fit_model(config, model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embtext = []\n","imgemb = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ClipCaptionModel(nn.Module):\n","    def __init__(self, config, prefix_length: int, prefix_size: int = 640, dist_loss=nn.MSELoss()):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","        self.gpt = T5ForConditionalGeneration.from_pretrained(config.decoder)\n","                                                #    eos_token_id=self.tokenizer.pad_token_id)\n","        self.gpt_embedding_size = self.gpt.get_input_embeddings().weight.shape[1]\n","        self.clip_project = QFormer(prefix_size, self.gpt_embedding_size,\n","                                    self.gpt_embedding_size * prefix_length)\n","        self.device = config.device\n","        self.dist_loss = dist_loss\n","        self.mlp = MLP(self.gpt_embedding_size, self.gpt_embedding_size)\n","\n","        for p in self.gpt.parameters():\n","            p.requires_grad = False\n","        for p in self.clip_model.parameters():\n","            p.requires_grad = False\n","\n","    def get_text_embeddings(self, tokens):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        return embedding_text\n","    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    @autocast(\"cuda\")\n","    def forward(self, query_tokens: torch.Tensor, query_mask: Optional[torch.Tensor],\n","                answer_tokens: torch.Tensor, answer_mask: Optional[torch.Tensor], image):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        embtext.append(embedding_text)\n","        imgemb.append(image)\n","        # print(image, embedding_text)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt(inputs_embeds=prefix_projections, labels=answer_tokens)\n","        return out, prefix_projections\n","\n","    def generate(self, image, texts, max_seq_len):\n","        tokens = torch.tensor(self.tokenizer.batch_encode_plus(texts, padding='max_length', max_length=max_seq_len, truncation=True)['input_ids'], dtype=torch.int64).to(self.device)\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt.generate(\n","            inputs_embeds=prefix_projections,\n","            max_new_tokens=self.prefix_length,\n","            no_repeat_ngram_size=3,\n","            repetition_penalty=2.,\n","        )\n","        res = [decode_question(x, self.tokenizer) for x in out]\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["prefix_length = 20\n","clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","gpt = T5ForConditionalGeneration.from_pretrained(config.decoder,\n","                                            eos_token_id=tokenizer.pad_token_id)\n","gpt_embedding_size = gpt.get_input_embeddings().weight.shape[1]\n","clip_project = QFormer(512, gpt_embedding_size,\n","                            gpt_embedding_size * prefix_length)\n","device = config.device\n","dist_loss=nn.MSELoss()\n","mlp = MLP(gpt_embedding_size, gpt_embedding_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["model = ClipCaptionModel(config, 20)\n","model = model.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1734 [00:20<?, ?it/s]\n"]}],"source":["# train_dataset = VQAv2_Dataset(config, dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_train_translation.jsonl\", imagespath_split=\"/home/jovyan/vqa_project/baselines/trainvqa/train2014/\", coef_size=0.5)\n","# train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","pbar = tqdm(train_loader, total=len(train_loader))\n","query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx = (0, 0, 0, 0, 0, 0)\n","for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","    query_tokens, query_mask, prefix = query_tokens.to(config.device), query_mask.to(config.device), prefix.to(\n","        config.device, dtype=torch.bfloat16)\n","    answer_tokens, answer_mask = answer_tokens.to(config.device), answer_mask.to(config.device)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query_tokens = query_tokens.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","gpt = T5ForConditionalGeneration.from_pretrained(config.decoder,\n","                                                   eos_token_id=tokenizer.pad_token_id)\n","gpt = gpt.to(config.device)\n","with torch.no_grad():\n","    embedding_text = gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","    embedding_text = embedding_text.last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([46928,   481, 14902,    35,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n","       device='cuda:1')"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["query_tokens[51]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[ 1.8356e-02, -2.0706e-02,  2.3954e-02,  ..., -2.7117e-02,\n","           2.3079e-04, -7.8800e-03],\n","         [-9.9768e-04,  7.8625e-03,  1.4143e-02,  ...,  2.6807e-02,\n","           2.7579e-02,  1.8058e-02],\n","         [ 1.6252e-02,  4.3728e-02,  2.5170e-02,  ..., -3.9186e-02,\n","          -2.2489e-02,  5.3207e-02],\n","         ...,\n","         [ 1.9278e-02,  2.0863e-02,  9.5985e-03,  ...,  2.1311e-03,\n","          -1.8911e-02,  4.7803e-02],\n","         [ 2.6116e-02,  9.5577e-03,  1.3170e-02,  ..., -6.9145e-03,\n","          -8.5817e-03,  3.9600e-02],\n","         [ 4.8260e-03,  1.7645e-02,  1.2168e-02,  ...,  3.2651e-03,\n","          -3.3427e-03,  4.1618e-02]],\n","\n","        [[ 3.0633e-02, -1.6978e-02, -3.9345e-03,  ..., -1.4538e-02,\n","           1.6281e-02, -5.5189e-03],\n","         [ 3.4946e-02, -1.5724e-03,  1.3459e-02,  ...,  3.1077e-03,\n","          -6.7513e-03,  1.2495e-02],\n","         [ 5.5245e-02,  1.3930e-02,  1.3249e-02,  ..., -2.1547e-02,\n","           2.7446e-03, -1.8740e-02],\n","         ...,\n","         [ 5.5332e-02,  1.4523e-02,  5.4831e-03,  ...,  6.2207e-03,\n","          -5.4252e-02,  8.2868e-02],\n","         [ 5.2831e-02,  9.5276e-03,  9.5130e-03,  ...,  3.4857e-03,\n","          -4.5850e-02,  6.6641e-02],\n","         [ 2.6635e-02,  1.2099e-02,  7.9323e-03,  ...,  9.9581e-03,\n","          -4.2657e-02,  5.3247e-02]],\n","\n","        [[ 2.5146e-04, -3.3940e-02,  1.4492e-03,  ..., -2.0355e-03,\n","          -1.5178e-03, -1.6454e-02],\n","         [-5.8909e-02, -2.8401e-02,  1.4027e-02,  ...,  2.9095e-02,\n","           1.7192e-02, -1.5503e-03],\n","         [-3.1388e-03,  3.1190e-02,  1.4206e-02,  ..., -1.7295e-02,\n","          -6.3056e-03, -1.2720e-02],\n","         ...,\n","         [ 2.4711e-02, -7.7518e-03, -6.5564e-03,  ...,  3.1485e-02,\n","          -6.8130e-02,  9.2300e-03],\n","         [ 2.6792e-02, -2.0448e-02, -7.9090e-03,  ...,  2.5334e-02,\n","          -6.5374e-02, -1.7990e-03],\n","         [ 5.2142e-03, -7.8331e-03, -3.9148e-03,  ...,  1.3203e-02,\n","          -6.3866e-02, -1.1397e-02]],\n","\n","        ...,\n","\n","        [[-2.5700e-04, -3.6247e-02,  4.5806e-03,  ..., -1.7904e-02,\n","          -5.3124e-03, -4.6806e-03],\n","         [-2.2147e-02,  1.7462e-02,  1.6729e-02,  ...,  3.1593e-03,\n","          -1.9980e-02,  7.7700e-03],\n","         [ 1.3794e-03,  5.1069e-02,  4.1991e-02,  ...,  3.7387e-02,\n","          -8.2407e-03,  2.3714e-02],\n","         ...,\n","         [ 1.7734e-02, -1.9417e-02,  2.2708e-02,  ...,  7.6404e-03,\n","          -2.4416e-02,  1.9772e-02],\n","         [ 1.1928e-02, -2.7074e-02,  2.4754e-02,  ...,  1.4139e-02,\n","          -2.5135e-02,  1.6671e-02],\n","         [ 1.2831e-02, -1.8039e-02,  2.5782e-02,  ...,  8.2956e-03,\n","          -2.3647e-02,  6.3777e-03]],\n","\n","        [[ 9.8265e-03, -3.4542e-02, -3.6114e-03,  ..., -2.1683e-02,\n","           1.7875e-02, -7.1630e-03],\n","         [-1.3800e-02,  4.0953e-02,  3.0868e-03,  ...,  1.4263e-02,\n","           1.5091e-02, -3.6224e-04],\n","         [-1.2777e-02,  1.4560e-02, -8.9041e-03,  ...,  1.2598e-02,\n","          -3.4886e-02, -3.6017e-02],\n","         ...,\n","         [ 2.2544e-02,  3.8998e-02, -3.7809e-03,  ..., -1.3345e-02,\n","          -4.5565e-02,  2.9826e-02],\n","         [ 1.5450e-02,  2.8398e-02,  2.0892e-04,  ..., -2.0470e-02,\n","          -4.0952e-02,  2.2508e-02],\n","         [ 9.0679e-03,  2.8757e-02, -6.7343e-03,  ..., -1.7420e-02,\n","          -2.0969e-02,  1.9853e-02]],\n","\n","        [[-2.9049e-02, -1.2713e-02, -9.6535e-03,  ...,  8.1955e-03,\n","          -2.3663e-02, -4.6614e-02],\n","         [ 1.2432e-02,  1.9580e-02,  5.2266e-03,  ...,  6.9126e-03,\n","          -4.6137e-02, -2.2797e-02],\n","         [ 2.3782e-02,  4.2073e-02,  7.5578e-03,  ..., -2.1696e-02,\n","          -6.1408e-03,  1.8603e-03],\n","         ...,\n","         [-3.8302e-04, -8.4590e-04, -1.2834e-02,  ..., -1.7762e-04,\n","          -5.2482e-04,  5.2937e-05],\n","         [ 4.9934e-02,  9.9474e-03,  4.4618e-03,  ...,  1.5062e-02,\n","          -2.8650e-02,  4.7236e-02],\n","         [ 2.9306e-02,  7.5509e-04,  1.8247e-03,  ...,  5.5561e-03,\n","          -4.0456e-02,  4.3698e-02]]], device='cuda:1')"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["embedding_text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[ 0.0089,  0.0030, -0.0128,  ...,  0.0012,  0.0230, -0.0087],\n","         [ 0.0262, -0.0093, -0.0084,  ...,  0.0046,  0.0234, -0.0168],\n","         [ 0.0163, -0.0115, -0.0106,  ...,  0.0041,  0.0231, -0.0080],\n","         ...,\n","         [ 0.0203, -0.0017, -0.0155,  ..., -0.0077,  0.0205, -0.0101],\n","         [ 0.0216, -0.0072, -0.0086,  ...,  0.0042,  0.0207, -0.0136],\n","         [ 0.0184, -0.0024, -0.0161,  ..., -0.0035,  0.0090, -0.0110]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n","\n","        ...,\n","\n","        [[ 0.0089,  0.0030, -0.0128,  ...,  0.0011,  0.0230, -0.0087],\n","         [ 0.0261, -0.0093, -0.0084,  ...,  0.0046,  0.0234, -0.0168],\n","         [ 0.0163, -0.0115, -0.0106,  ...,  0.0041,  0.0231, -0.0080],\n","         ...,\n","         [ 0.0203, -0.0017, -0.0155,  ..., -0.0077,  0.0205, -0.0101],\n","         [ 0.0217, -0.0072, -0.0086,  ...,  0.0042,  0.0208, -0.0136],\n","         [ 0.0184, -0.0024, -0.0161,  ..., -0.0035,  0.0090, -0.0110]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:1', dtype=torch.float16, grad_fn=<ViewBackward0>)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["proj"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lm_text='<LM>Принялся Кутузов рассказывать свою историю как он сюда попал. Началось'\n","input_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\n","outputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\n","print(tokenizer.decode(outputs[0][1:]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[ 0.0089,  0.0030, -0.0128,  ...,  0.0012,  0.0230, -0.0087],\n","         [ 0.0262, -0.0093, -0.0084,  ...,  0.0046,  0.0234, -0.0168],\n","         [ 0.0163, -0.0115, -0.0106,  ...,  0.0041,  0.0231, -0.0080],\n","         ...,\n","         [ 0.0203, -0.0017, -0.0155,  ..., -0.0077,  0.0205, -0.0101],\n","         [ 0.0216, -0.0072, -0.0086,  ...,  0.0042,  0.0207, -0.0136],\n","         [ 0.0184, -0.0024, -0.0161,  ..., -0.0035,  0.0090, -0.0110]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n","\n","        ...,\n","\n","        [[ 0.0089,  0.0030, -0.0128,  ...,  0.0011,  0.0230, -0.0087],\n","         [ 0.0261, -0.0093, -0.0084,  ...,  0.0046,  0.0234, -0.0168],\n","         [ 0.0163, -0.0115, -0.0106,  ...,  0.0041,  0.0231, -0.0080],\n","         ...,\n","         [ 0.0203, -0.0017, -0.0155,  ..., -0.0077,  0.0205, -0.0101],\n","         [ 0.0217, -0.0072, -0.0086,  ...,  0.0042,  0.0208, -0.0136],\n","         [ 0.0184, -0.0024, -0.0161,  ..., -0.0035,  0.0090, -0.0110]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n","\n","        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:1', dtype=torch.float16, grad_fn=<ViewBackward0>)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["proj"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gpt = gpt.to(config.device)\n","outputs = gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","embeddings = outputs.last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["model = ClipCaptionModel(config, config.prefix_length)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = model.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"r.nvmlDeviceGetNvLinkRemoteDeviceType_ INTERNAL ASSERT FAILED at \"../c10/cuda/driver_api.cpp\":27, please report a bug to PyTorch. Can't find nvmlDeviceGetNvLinkRemoteDeviceType: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetNvLinkRemoteDeviceType","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m outputs, proj \u001b[39m=\u001b[39m model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;32m/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m@autocast\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, query_tokens: torch\u001b[39m.\u001b[39mTensor, query_mask: Optional[torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m             answer_tokens: torch\u001b[39m.\u001b[39mTensor, answer_mask: Optional[torch\u001b[39m.\u001b[39mTensor], image):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m         embedding_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mforward(input_ids\u001b[39m=\u001b[39;49mquery_tokens, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         embedding_text \u001b[39m=\u001b[39m embedding_text\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/fredt5-large_fullilya.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_model\u001b[39m.\u001b[39mencode_image(image)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1093\u001b[0m         layer_module\u001b[39m.\u001b[39mforward,\n\u001b[1;32m   1094\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         output_attentions,\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1108\u001b[0m         hidden_states,\n\u001b[1;32m   1109\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1110\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1111\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1112\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1113\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1114\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1115\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1116\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1117\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1118\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1121\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:747\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    744\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    746\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    749\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:336\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    335\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 336\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    337\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:304\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 304\u001b[0m     hidden_gelu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi_0(hidden_states))\n\u001b[1;32m    305\u001b[0m     hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_1(hidden_states)\n\u001b[1;32m    306\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(math\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39mpi) \u001b[39m*\u001b[39m (\u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mpow(\u001b[39minput\u001b[39;49m, \u001b[39m3.0\u001b[39;49m))))\n","\u001b[0;31mRuntimeError\u001b[0m: r.nvmlDeviceGetNvLinkRemoteDeviceType_ INTERNAL ASSERT FAILED at \"../c10/cuda/driver_api.cpp\":27, please report a bug to PyTorch. Can't find nvmlDeviceGetNvLinkRemoteDeviceType: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetNvLinkRemoteDeviceType"]}],"source":["outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4978927,"sourceId":8374280,"sourceType":"datasetVersion"},{"datasetId":4982204,"sourceId":8378551,"sourceType":"datasetVersion"},{"datasetId":4982248,"sourceId":8378604,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
