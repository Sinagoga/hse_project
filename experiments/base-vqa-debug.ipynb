{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-13T23:14:37.534352Z","iopub.status.busy":"2024-05-13T23:14:37.533955Z","iopub.status.idle":"2024-05-13T23:15:01.119580Z","shell.execute_reply":"2024-05-13T23:15:01.118546Z","shell.execute_reply.started":"2024-05-13T23:14:37.534318Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as nnf\n","from torch.amp import autocast\n","from torch import einsum\n","import torch.nn.functional as F\n","\n","import open_clip\n","\n","from transformers import GPT2LMHeadModel, AutoTokenizer\n","\n","from typing import Optional\n","\n","from transformers.optimization import Adafactor\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","import pickle\n","from torchmetrics.text import BLEUScore\n","from evaluate import load\n","from statistics import mean\n","from einops import rearrange\n","import math\n","import wandb\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:16:19.172110Z","iopub.status.busy":"2024-05-13T23:16:19.171722Z","iopub.status.idle":"2024-05-13T23:16:19.181270Z","shell.execute_reply":"2024-05-13T23:16:19.180155Z","shell.execute_reply.started":"2024-05-13T23:16:19.172083Z"},"trusted":true},"outputs":[],"source":["def exists(val):\n","    return val is not None\n","\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","def expand_mask(mask):\n","    assert mask.ndim > 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n","    if mask.ndim == 3:\n","        mask = mask.unsqueeze(1)\n","    while mask.ndim < 4:\n","        mask = mask.unsqueeze(0)\n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:16:19.741261Z","iopub.status.busy":"2024-05-13T23:16:19.740880Z","iopub.status.idle":"2024-05-13T23:16:19.755640Z","shell.execute_reply":"2024-05-13T23:16:19.754590Z","shell.execute_reply.started":"2024-05-13T23:16:19.741232Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, input_dim, dim_embedds, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert dim_embedds % num_heads == 0\n","\n","        self.dim_embedds = dim_embedds\n","        self.num_heads = num_heads\n","        self.d_k = dim_embedds // num_heads\n","\n","        self.W_qkv = nn.Linear(input_dim, 3 * dim_embedds)\n","        self.W_o = nn.Linear(dim_embedds, dim_embedds)\n","        self.dropout = nn.Dropout(0)\n","        self._reset_parameters()\n","    def _reset_parameters(self):\n","        nn.init.xavier_uniform_(self.W_qkv.weight)\n","        self.W_qkv.bias.data.fill_(0)\n","        nn.init.xavier_uniform_(self.W_o.weight)\n","        self.W_o.bias.data.fill_(0)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","            _MASKING_VALUE = -1e+30 if attn_scores.dtype == torch.float32 else -1e+4\n","            attn_scores = attn_scores.masked_fill(mask == 0, _MASKING_VALUE)\n","        attention = torch.softmax(attn_scores, dim=-1)\n","        # print(V)\n","        # output = torch.matmul(self.dropout(attention), V)\n","        output = torch.matmul(attention, V)\n","        return output, attention\n","\n","    def combine_heads(self, x, batch_size, seq_length):\n","        return x.permute(0, 2, 1, 3).reshape(batch_size, seq_length, self.dim_embedds)\n","\n","    def forward(self, x, mask=None, return_attn=False):\n","        batch_size, seq_length, _ = x.size()\n","\n","        if mask is not None:\n","            mask = expand_mask(mask)\n","        # if (x.size() == torch.Size([24, 1, 512])):\n","          #  print(\"IMAGEFIX: \")\n","          #  self.IMAGEFIX(x)\n","          #  print(\"DONE\")\n","        QKV = self.W_qkv(x)\n","        QKV = QKV.reshape(batch_size, seq_length, self.num_heads, 3 * self.d_k)\n","        QKV = QKV.permute(0, 2, 1, 3)\n","        q, k, v = QKV.chunk(3, dim=-1)\n","        attn_output, attention = self.scaled_dot_product_attention(q,k,v, mask)\n","        # attn_output = attn_output.permute(0, 2, 1, 3)\n","        # attn_output = attn_output.reshape(batch_size, seq_length, self.dim_embedds)\n","\n","        output = self.W_o(self.combine_heads(attn_output, batch_size, seq_length))\n","        if return_attn:\n","            return output, attention\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:16:20.786452Z","iopub.status.busy":"2024-05-13T23:16:20.785614Z","iopub.status.idle":"2024-05-13T23:16:20.795374Z","shell.execute_reply":"2024-05-13T23:16:20.794245Z","shell.execute_reply.started":"2024-05-13T23:16:20.786418Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module): #MLP\n","    def __init__(self, inp_shape, output_shape, act=nn.ReLU):\n","        super(FeedForward, self).__init__()\n","        self.seq = nn.Sequential(\n","            nn.Linear(inp_shape, inp_shape*2),\n","            act(),\n","            nn.Linear(inp_shape*2, output_shape)\n","        )\n","    @autocast(\"cuda\")\n","    def forward(self, x):\n","        return self.seq(x)\n","class TextFeedForward(nn.Module):\n","    def __init__(self, text_emb_size, output_size, act=nn.ReLU):\n","        super(TextFeedForward, self).__init__()\n","        self.seq = nn.Sequential(\n","            nn.Linear(text_emb_size, text_emb_size*2),\n","            act(),\n","            nn.Linear(text_emb_size*2, text_emb_size*2),\n","            act(),\n","            nn.Linear(text_emb_size*2, output_size)\n","        )\n","    def forward(self, x):\n","        return self.seq(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:16:21.202610Z","iopub.status.busy":"2024-05-13T23:16:21.202232Z","iopub.status.idle":"2024-05-13T23:16:21.210441Z","shell.execute_reply":"2024-05-13T23:16:21.209401Z","shell.execute_reply.started":"2024-05-13T23:16:21.202579Z"},"trusted":true},"outputs":[],"source":["class QFormerBlock(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, output_size, bias=True):\n","        super(QFormerBlock, self).__init__()\n","        self.attn = MultiHeadAttention(text_emb_size, text_emb_size, 16)\n","        # self.cross_attn = MultiHeadAttention(img_emb_size, img_emb_size, num_heads=16)\n","        self.cross_attn = BidirectionalCrossAttention(\n","            dim=img_emb_size,\n","            heads=16,\n","            dim_head=1024,\n","            context_dim=text_emb_size\n","        )\n","        self.text_feed_forward = TextFeedForward(text_emb_size, output_size)\n","\n","        # print(\"QFO RMER IMG \" , str(img_emb_size), \"TEXT\", text_emb_size)\n","    @autocast(\"cuda\")\n","    def forward(self, img_emb: torch.Tensor, text_emb: torch.Tensor) -> torch.Tensor:\n","        text_emb = self.attn(text_emb)\n","        # print(\"QFORMER: да \", img_emb.reshape(-1, 1, img_emb.shape[1]).size(), text_emb.size())\n","        img_emb, text_emb = self.cross_attn(img_emb.reshape(-1, 1, img_emb.shape[1]), text_emb)\n","        # print(\"QFORMER все\")\n","        text_emb = self.text_feed_forward(text_emb)\n","        return img_emb, text_emb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:16:21.996668Z","iopub.status.busy":"2024-05-13T23:16:21.996028Z","iopub.status.idle":"2024-05-13T23:16:22.003333Z","shell.execute_reply":"2024-05-13T23:16:22.002325Z","shell.execute_reply.started":"2024-05-13T23:16:21.996635Z"},"trusted":true},"outputs":[],"source":["class Blocks(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, n_blocks):\n","        super(Blocks, self).__init__()\n","        self.model = nn.Sequential(*[QFormerBlock(img_emb_size, text_emb_size, text_emb_size) for _ in range(n_blocks)])\n","    def forward(self, *x):\n","        for block in self.model._modules.values():\n","          x = block(*x)\n","          if x[0].shape[1] == 1:\n","            x = (x[0][:, 0, :], x[1])\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:16:22.682637Z","iopub.status.busy":"2024-05-13T23:16:22.681943Z","iopub.status.idle":"2024-05-13T23:16:22.690045Z","shell.execute_reply":"2024-05-13T23:16:22.689091Z","shell.execute_reply.started":"2024-05-13T23:16:22.682604Z"},"trusted":true},"outputs":[],"source":["class QFormer(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, output_size, n_blocks=4, bias=True):\n","        super(QFormer, self).__init__()\n","\n","        self.blocks = Blocks(img_emb_size, text_emb_size, n_blocks)\n","        self.res = nn.Linear(img_emb_size + text_emb_size, output_size)\n","\n","    @autocast(\"cuda\")\n","    def forward(self, img_emb, text_emb):\n","        img_emb, text_emb = self.blocks(img_emb, text_emb)\n","        text_emb = text_emb.mean(axis=1)\n","        res_emb = torch.cat((img_emb, text_emb), axis=1)\n","        res_emb = self.res(res_emb)\n","        return res_emb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:53:32.021352Z","iopub.status.busy":"2024-05-13T23:53:32.020380Z","iopub.status.idle":"2024-05-13T23:53:32.040583Z","shell.execute_reply":"2024-05-13T23:53:32.039588Z","shell.execute_reply.started":"2024-05-13T23:53:32.021318Z"},"trusted":true},"outputs":[],"source":["class ClipCaptionModel(nn.Module):\n","    def __init__(self, config, prefix_length: int, prefix_size: int = 512, dist_loss=nn.MSELoss()):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","        self.gpt = GPT2LMHeadModel.from_pretrained(config.decoder,\n","                                                   eos_token_id=self.tokenizer.pad_token_id)\n","        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n","        self.clip_project = QFormer(prefix_size, self.gpt_embedding_size,\n","                                    self.gpt_embedding_size * prefix_length)\n","        self.device = config.device\n","        self.dist_loss = dist_loss\n","        self.mlp = FeedForward(self.gpt_embedding_size, self.gpt_embedding_size)\n","\n","        for p in self.gpt.parameters():\n","            p.requires_grad = False\n","        for p in self.clip_model.parameters():\n","            p.requires_grad = False\n","\n","    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    @autocast(\"cuda\")\n","    def forward(self, query_tokens: torch.Tensor, query_mask: Optional[torch.Tensor],\n","                answer_tokens: torch.Tensor, answer_mask: Optional[torch.Tensor], image):\n","        embedding_text = self.gpt.transformer.wte(query_tokens)\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt(inputs_embeds=prefix_projections, labels=answer_tokens)\n","        return out, prefix_projections\n","\n","    def generate(self, image, texts, max_seq_len):\n","        # tokens = torch.tensor(self.tokenizer.batch_encode_plus(texts, )['input_ids'], dtype=torch.int64).to(self.device)\n","        tokens = torch.tensor(self.tokenizer.batch_encode_plus(texts, padding='max_length', max_length=max_seq_len, truncation=True)['input_ids'], dtype=torch.int64).to(self.device)\n","        embedding_text = self.gpt.transformer.wte(tokens)\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt.generate(\n","            inputs_embeds=prefix_projections,\n","            max_new_tokens=self.prefix_length,\n","            no_repeat_ngram_size=3,\n","            repetition_penalty=2.,\n","        )\n","        res = [decode_question(x, self.tokenizer) for x in out]\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:10.238419Z","iopub.status.busy":"2024-05-12T16:35:10.237459Z","iopub.status.idle":"2024-05-12T16:35:12.132381Z","shell.execute_reply":"2024-05-12T16:35:12.131436Z","shell.execute_reply.started":"2024-05-12T16:35:10.238377Z"},"trusted":true},"outputs":[],"source":["bertscore = load(\"bertscore\")\n","meteor = load('meteor')\n","rouge = load('rouge')\n","bleu_scorers = [BLEUScore(n_gram=i) for i in [1, 2, 3]] + [bertscore, meteor, rouge]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:14.372918Z","iopub.status.busy":"2024-05-12T16:35:14.372562Z","iopub.status.idle":"2024-05-12T16:35:46.471596Z","shell.execute_reply":"2024-05-12T16:35:46.470408Z","shell.execute_reply.started":"2024-05-12T16:35:14.372887Z"},"trusted":true},"outputs":[],"source":["wandb.login(key=\"\")\n","wandb.init(project=\"\", sync_tensorboard=True, name=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:17:48.923946Z","iopub.status.busy":"2024-05-13T23:17:48.923232Z","iopub.status.idle":"2024-05-13T23:17:48.929456Z","shell.execute_reply":"2024-05-13T23:17:48.928365Z","shell.execute_reply.started":"2024-05-13T23:17:48.923911Z"},"trusted":true},"outputs":[],"source":["def decode_question(question_token, tokenizer):\n","    decoded_string = tokenizer.decode(question_token)\n","    if \"<pad>\" in decoded_string:\n","        truncate_pads = decoded_string.index(\"<pad>\")\n","        decoded_string = decoded_string[:truncate_pads]\n","    return decoded_string"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:17:52.010474Z","iopub.status.busy":"2024-05-13T23:17:52.009843Z","iopub.status.idle":"2024-05-13T23:17:52.021926Z","shell.execute_reply":"2024-05-13T23:17:52.020878Z","shell.execute_reply.started":"2024-05-13T23:17:52.010442Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def train(model, optimizer, scheduler, loss_func, loader, epoch, args):\n","    model.train()\n","    pbar = tqdm(loader, total=len(loader))\n","    step = 0\n","    for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","\n","        query_tokens, query_mask, prefix = query_tokens.to(args.device), query_mask.to(args.device), prefix.to(\n","            args.device, dtype=torch.bfloat16)\n","        answer_tokens, answer_mask = answer_tokens.to(args.device), answer_mask.to(args.device)\n","        # print(query_tokens.size(), query_mask.size(), answer_tokens.size(), answer_mask.size(), prefix.size())\n","        outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","        logits = outputs.logits\n","        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), answer_tokens.flatten().to(torch.int64),\n","                                 ignore_index=0)\n","\n","        loss2 = model.dist_loss(model.gpt.transformer.wte(answer_tokens).to(torch.float32), proj.to(torch.float32))\n","        loss += loss2\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n","\n","        #backpropogation\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        pbar.set_postfix({\"loss\": loss.item(), \"dist_loss\": loss2.item()})\n","        wandb.log({\"loss\": loss.item(), \"dist_loss\": loss2.item()})\n","        step += 1\n","        if step % 1000 == 0:\n","            print(\"TEXT:\", train_dataset.tokenizer.decode(answer_tokens[0]))\n","            print(\"PREDICTED: \", model.generate(torch.tensor([train_dataset[idx[0]][4].tolist()]).to(args.device),\n","                                                [decode_question(query_tokens[0], model.tokenizer)], train_dataset.max_seq_len)[0])\n","    with open(f'{args.save_path}checkpoint_{epoch}.pkl', 'wb') as f:\n","        pickle.dump(model, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:46.499711Z","iopub.status.busy":"2024-05-12T16:35:46.499126Z","iopub.status.idle":"2024-05-12T16:35:46.521515Z","shell.execute_reply":"2024-05-12T16:35:46.520544Z","shell.execute_reply.started":"2024-05-12T16:35:46.499679Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, optimizer, scheduler, loss_func, loader, args):\n","    model.eval()\n","    pbar = tqdm(loader, total=len(loader))\n","    step = 0\n","\n","    bl1 = []\n","    bl2 = []\n","    bl3 = []\n","    brt = []\n","    mtr = []\n","    rg = []\n","    val_losses = []\n","    val_dist = []\n","    for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","        query_tokens, query_mask, prefix = query_tokens.to(args.device), query_mask.to(args.device), prefix.to(\n","            args.device, dtype=torch.bfloat16)\n","        answer_tokens, answer_mask = answer_tokens.to(args.device), answer_mask.to(args.device)\n","        outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","        logits = outputs.logits\n","        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), answer_tokens.flatten().to(torch.int64),\n","                                 ignore_index=0)\n","        loss2 = model.dist_loss(model.gpt.transformer.wte(answer_tokens), proj)\n","\n","        # real = model.tokenizer.batch_decode(answer_tokens)\n","        real = [decode_question(answer_tokens[i], model.tokenizer) for i in range(len(answer_tokens))]\n","#         pred = model.generate(torch.tensor([val_dataset[idx[j]][4].tolist() for j in range(len(idx))]).to(args.device),\n","#                               [\"Что на картинке?\" for _ in range(len(idx))])\n","        pred = model.generate(torch.tensor([val_dataset[idx[j]][4].tolist() for j in range(len(idx))]).to(args.device),\n","                              [decode_question(query_tokens[j], model.tokenizer) for j in range(len(idx))], val_dataset.max_seq_len)\n","        \n","#         model.generate(torch.tensor([train_dataset[idx[0]][4].tolist()]).to(args.device),\n","#                                                 decode_question(query_tokens, model.tokenizer))[0]\n","        \n","        # real = truncate_sentences(real)\n","        # pred = truncate_sentences(pred)\n","        \n","        bl1.append(bleu_scorers[0](pred, real))\n","        bl2.append(bleu_scorers[1](pred, real))\n","        bl3.append(bleu_scorers[2](pred, real))\n","        brt.append(bleu_scorers[3].compute(predictions=pred, references=real, lang=\"ru\")['f1'])\n","        mtr.append(bleu_scorers[4].compute(predictions=pred, references=real)['meteor'])\n","        rg.append(bleu_scorers[5].compute(predictions=pred, references=real)['rougeL'])\n","\n","        if step % 400 == 0:\n","            print(\"TEXT:\", real[0])\n","            print(\"PREDICTED: \", pred[0])\n","\n","            imgs = []\n","            for j in range(len(idx)):\n","                wa_img = wandb.Image(\n","                    val_dataset.get_image(idx[j]),\n","                    caption=f\"REAL : {real[j]}, PREDICTED : {pred[j]}\"\n","                )\n","                imgs.append(wa_img)\n","\n","            wandb.log({\"Generations.\": imgs})\n","\n","        step += 1\n","\n","        pbar.set_postfix({\"val_loss\": loss.item(), \"val_dist\": loss2.item()})\n","        val_losses.append(loss.item())\n","        val_dist.append(loss2.item())\n","\n","    wandb.log({\"val_loss\": mean(val_losses),\n","               \"val_dist\": mean(val_dist)})\n","\n","    # wandb.log({\n","    #     \"bleu_1\": mean([tensor.item() for tensor in bl1]),\n","    #     \"bleu_2\": mean([tensor.item() for tensor in bl2]),\n","    #     \"bleu_3\": mean([tensor.item() for tensor in bl3]),\n","    #     \"bert_score\": np.mean(np.mean([tensor for tensor in brt])),\n","    #     \"meteor_score\": np.mean([tensor for tensor in mtr]),\n","    #     \"rouge_score\": np.mean([tensor for tensor in rg])\n","    # })\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:36:04.797205Z","iopub.status.busy":"2024-05-12T16:36:04.796511Z","iopub.status.idle":"2024-05-12T16:36:04.809456Z","shell.execute_reply":"2024-05-12T16:36:04.808123Z","shell.execute_reply.started":"2024-05-12T16:36:04.797173Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def fit_model(args, model, train_loader, val_loader):\n","    wandb.config = {\n","        \"learning_rate\": args.learning_rate,\n","        \"epochs\": args.num_epochs,\n","        \"batch_size\": args.batch_size\n","    }\n","\n","    # if not os.path.exists(args.save_path):\n","    #     os.makedirs(args.save_path)\n","    device = args.device\n","\n","    # model = ClipCaptionModel(args, args.prefix_length)\n","    model = model.to(args.device)\n","\n","    wandb.watch(model, log_freq=10, log=\"gradients\")\n","\n","    model.train()\n","\n","    loss_func = nn.CrossEntropyLoss()\n","    optimizer = Adafactor(model.parameters(), lr=args.learning_rate,\n","                          relative_step=False  # for adafactor\n","                          )\n","\n","    # train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","    # val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=15000\n","    )\n","    print(\"ZERO SHOT\")\n","    evaluate(model, optimizer, scheduler, loss_func, val_loader, args)\n","    print(\"Start train model\")\n","    for epoch in range(4, args.num_epochs):\n","        if epoch == args.frozen_gpt:\n","            print(\"GPT UNFROZEN\")\n","            for p in model.gpt.parameters():\n","                p.requires_grad = True\n","        if epoch == args.frozen_clip:\n","            print(\"CLIP UNFROZEN\")\n","            for p in model.clip_model.parameters():\n","                p.requires_grad = True\n","        print(f\"---------- Train epoch {epoch} ---------\")\n","        train(model, optimizer, scheduler, loss_func, train_loader, epoch, args)\n","        print(f\"---------- Evaluate epoch {epoch} ---------\")\n","        evaluate(model, optimizer, scheduler, loss_func, val_loader, args)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:18:13.833872Z","iopub.status.busy":"2024-05-13T23:18:13.833039Z","iopub.status.idle":"2024-05-13T23:18:22.979012Z","shell.execute_reply":"2024-05-13T23:18:22.978052Z","shell.execute_reply.started":"2024-05-13T23:18:13.833841Z"},"trusted":true},"outputs":[],"source":["config = Config()\n","train_dataset = VQAv2_Dataset(config, dataset_path=\"VQAv2_train_translation.jsonl\", imagespath_split=\"trainvqa/train2014/\", coef_size=1)\n","val_dataset = VQAv2_Dataset(config, dataset_path=\"VQAv2_val_translation.jsonl\", imagespath_split=\"valvqa/val2014/\", coef_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:53:37.689100Z","iopub.status.busy":"2024-05-13T23:53:37.688719Z","iopub.status.idle":"2024-05-13T23:53:47.886538Z","shell.execute_reply":"2024-05-13T23:53:47.885546Z","shell.execute_reply.started":"2024-05-13T23:53:37.689075Z"},"trusted":true},"outputs":[],"source":["# model = ClipCaptionModel(config, config.prefix_length)\n","model = pickle.load(open(\"/saved_models/checkpoint_3.pkl\", 'rb'))\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_model(config, model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:15:01.123316Z","iopub.status.busy":"2024-05-13T23:15:01.122666Z","iopub.status.idle":"2024-05-13T23:15:01.129588Z","shell.execute_reply":"2024-05-13T23:15:01.128709Z","shell.execute_reply.started":"2024-05-13T23:15:01.123285Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    encoder: str = \"ViT-B-16\"\n","    decoder: str = \"ai-forever/rugpt3medium_based_on_gpt2\"\n","    batch_size: int = 1024\n","    num_epochs: int = 100\n","    frozen_gpt: int = 20\n","    frozen_clip: int = 60\n","    learning_rate: float  = 2e-4\n","    save_path: str = \"model_saves/\"\n","    prefix_length: int = 20\n","    only_prefix: int = False\n","    prefix: str = \"prefix_small\"\n","    device: str = \"cuda:1\"\n","    save_every: int = 1\n","    warmup_steps: int = 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:15:01.131102Z","iopub.status.busy":"2024-05-13T23:15:01.130797Z","iopub.status.idle":"2024-05-13T23:15:01.160175Z","shell.execute_reply":"2024-05-13T23:15:01.159164Z","shell.execute_reply.started":"2024-05-13T23:15:01.131077Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import sys\n","from matplotlib import pyplot as plt\n","import json\n","from PIL import Image\n","class VQAv2_Dataset(Dataset):\n","    def __init__(self, config, dataset_path, coef_size=0.1,\n","                 tokenizer_name=\"\", prefix_length=20, normalize_prefix=False, imagespath_split=None):\n","        if not tokenizer_name:\n","            tokenizer_name = config.decoder\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        clip_model, _, self.preprocess = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.prefix_length = prefix_length\n","        self.normalize_prefix = normalize_prefix\n","\n","        with open(dataset_path, 'r') as f:\n","            dataset = json.loads(list(f)[0])\n","\n","        self.img_paths = []\n","        self.query_tokens = []\n","        self.answer_tokens = []\n","\n","        max_img = len(dataset)*coef_size\n","        for i, el in tqdm(enumerate(dataset), total=max_img):\n","            answer = el['answer'] \n","            question = el['question']\n","            self.query_tokens += [torch.tensor(self.tokenizer.encode(question), dtype=torch.int64)]\n","            self.answer_tokens += [torch.tensor(self.tokenizer.encode(answer), dtype=torch.int64)]\n","            if (\"val\" in imagespath_split):\n","                self.img_paths += [imagespath_split + el['image_id'].replace(\"train\", \"val\") + \".jpg\"]\n","            else:\n","                self.img_paths += [imagespath_split + el['image_id'] + \".jpg\"]\n","            if int(i) >= max_img:\n","                  break\n","        del dataset\n","        sys.stdout.flush()\n","\n","        #all_len\n","        self.max_seq_len = prefix_length\n","        # self.type = data_type\n","\n","    \"\"\"Почему не паддили captions?\"\"\"\n","    def pad_tokens(self, item: int):\n","        query_tokens = self.query_tokens[item]\n","        padding = self.max_seq_len - query_tokens.shape[0]\n","        if padding > 0:\n","            query_tokens = torch.cat((query_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n","            self.query_tokens[item] = query_tokens\n","        elif padding < 0:\n","            query_tokens = query_tokens[:self.max_seq_len]\n","            self.query_tokens[item] = query_tokens\n","        query_mask = query_tokens.ge(0)  # mask is zero where we out of sequence\n","        query_tokens[~query_mask] = 0\n","        query_mask = query_mask.float()\n","\n","\n","        answer_tokens = self.answer_tokens[item]\n","        padding = self.max_seq_len - answer_tokens.shape[0]\n","        if padding > 0:\n","            answer_tokens = torch.cat((answer_tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n","            self.answer_tokens[item] = answer_tokens\n","        elif padding < 0:\n","            answer_tokens = answer_tokens[:self.max_seq_len]\n","            self.answer_tokens[item] = answer_tokens\n","        answer_mask = answer_tokens.ge(0)  # mask is zero where we out of sequence\n","        answer_tokens[~answer_mask] = 0\n","        answer_mask = answer_mask.float()\n","\n","        return query_tokens, query_mask, answer_tokens, answer_mask\n","\n","    def get_image(self, item):\n","        name = str(self.img_paths[item])\n","        # name = f\"{self.img_path}/{name}\"\n","        image_resized = Image.open(name)\n","        image_resized = image_resized.resize((256, 256))\n","        return image_resized\n","        # image_resized = cv2.resize(self.image_idx[item], (256,256))\n","        # return Image.fromarray(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))\n","\n","    def __len__(self) -> int:\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, item):\n","        image = self.get_image(item)\n","        image = self.preprocess(image).unsqueeze(0)\n","        query_tokens, query_mask, answer_tokens, answer_mask = self.pad_tokens(item)\n","        return query_tokens, query_mask, answer_tokens, answer_mask, image[0], item\n","        # return query_tokens, query_mask, answer_tokens, answer_mask, item\n","\n","    def show_image(self, item):\n","        image = self.get_image(item)\n","        text = self.tokenizer.decode(self.pad_tokens(item)[2])\n","        plt.imshow(image)\n","        print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:15:01.163151Z","iopub.status.busy":"2024-05-13T23:15:01.162582Z","iopub.status.idle":"2024-05-13T23:15:01.181903Z","shell.execute_reply":"2024-05-13T23:15:01.180932Z","shell.execute_reply.started":"2024-05-13T23:15:01.163118Z"},"trusted":true},"outputs":[],"source":["class BidirectionalCrossAttention(nn.Module):\n","    def __init__(\n","            self,\n","            *,\n","            dim,\n","            heads=8,\n","            dim_head=64,\n","            context_dim=None,\n","            dropout=0.,\n","            talking_heads=False,\n","            prenorm=False,\n","    ):\n","        super().__init__()\n","        context_dim = default(context_dim, dim)\n","        self.norm = nn.LayerNorm(dim) if prenorm else nn.Identity()\n","        self.context_norm = nn.LayerNorm(context_dim) if prenorm else nn.Identity()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        inner_dim = dim_head * heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.context_dropout = nn.Dropout(dropout)\n","        self.to_qk = nn.Linear(dim, inner_dim, bias=False)\n","        self.context_to_qk = nn.Linear(context_dim, inner_dim, bias=False)\n","        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n","        self.context_to_v = nn.Linear(context_dim, inner_dim, bias=False)\n","        self.to_out = nn.Linear(inner_dim, dim)\n","        self.context_to_out = nn.Linear(inner_dim, context_dim)\n","        self.talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else nn.Identity()\n","        self.context_talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else nn.Identity()\n","\n","    def stable_softmax(self, t, dim=-1):\n","        t = t - t.amax(dim=dim, keepdim=True)\n","        return t.softmax(dim=dim)\n","\n","    def forward(\n","            self,\n","            x,\n","            context,\n","            mask=None,\n","            context_mask=None,\n","            return_attn=False,\n","            rel_pos_bias=None\n","    ):\n","\n","        b, i, j, h, device = x.shape[0], x.shape[-2], context.shape[-2], self.heads, x.device\n","        x = self.norm(x)\n","        context = self.context_norm(context)\n","        qk, v = self.to_qk(x), self.to_v(x)\n","        context_qk, context_v = self.context_to_qk(context), self.context_to_v(context)\n","        qk, context_qk, v, context_v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h),\n","                                           (qk, context_qk, v, context_v))\n","        sim = einsum('b h i d, b h j d -> b h i j', qk, context_qk) * self.scale\n","        if exists(rel_pos_bias):\n","            sim = sim + rel_pos_bias\n","        if exists(mask) or exists(context_mask):\n","            mask = default(mask, torch.ones((b, i), device=device, dtype=torch.bool))\n","            context_mask = default(context_mask, torch.ones((b, j), device=device, dtype=torch.bool))\n","            attn_mask = rearrange(mask, 'b i -> b 1 i 1') * rearrange(context_mask, 'b j -> b 1 1 j')\n","            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n","        attn = self.stable_softmax(sim, dim=-1)\n","        context_attn = self.stable_softmax(sim, dim=-2)\n","        attn = self.dropout(attn)\n","        context_attn = self.context_dropout(context_attn)\n","        attn = self.talking_heads(attn)\n","        context_attn = self.context_talking_heads(context_attn)\n","        out = einsum('b h i j, b h j d -> b h i d', attn, context_v)\n","        context_out = einsum('b h j i, b h j d -> b h i d', context_attn, v)\n","        out, context_out = map(lambda t: rearrange(t, 'b h n d -> b n (h d)'), (out, context_out))\n","        out = self.to_out(out)\n","        context_out = self.context_to_out(context_out)\n","\n","        if return_attn:\n","            return out, context_out, attn, context_attn\n","        return out, context_out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["2"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4978927,"sourceId":8374280,"sourceType":"datasetVersion"},{"datasetId":4982204,"sourceId":8378551,"sourceType":"datasetVersion"},{"datasetId":4982248,"sourceId":8378604,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
