{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from torchmetrics.text import BLEUScore\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"/home/jovyan/vqa_project/baselines/finetuning/blip_vqa_base_tune\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, processor, imagespath_split):\n",
    "        # self.dataset = dataset\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.dataset = json.loads(list(f)[0])\n",
    "        self.processor = processor\n",
    "        self.imagespath_split = imagespath_split\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get image + text\n",
    "        question = self.dataset[idx]['question']\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        if (\"val\" in self.imagespath_split):\n",
    "            image_path = self.imagespath_split + self.dataset[idx]['image_id'].replace(\"train\", \"val\") + \".jpg\"\n",
    "        else:\n",
    "            image_path = self.imagespath_split + self.dataset[idx]['image_id'] + \".jpg\"\n",
    "        # image_id = self.dataset[idx]['pid']\n",
    "        # image_path = f\"Data/train_fill_in_blank/{image_id}/image.png\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # image = image.resize((224, 224))\n",
    "        text = question\n",
    "        \n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\",  max_length=60)\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, padding='max_length', truncation=True, max_length=20, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        # labels = self.processor.tokenizer(answer, padding='max_length', truncation=True, max_length=8, return_tensors='pt')['input_ids']\n",
    "\n",
    "        encoding[\"labels\"] = labels\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():  encoding[k] = v.squeeze()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VQADataset(dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_train_translation.jsonl\",\n",
    "                          processor=processor,\n",
    "                          imagespath_split=\"/home/jovyan/vqa_project/baselines/trainvqa/train2014/\")\n",
    "valid_dataset = VQADataset(dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_val_translation.jsonl\",\n",
    "                          processor=processor,\n",
    "                          imagespath_split=\"/home/jovyan/vqa_project/baselines/valvqa/val2014/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 36\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=20)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=24, shuffle=False, pin_memory=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"278590c2621521efe866317352d7f3e13fef885f\")\n",
    "wandb.init(project=\"blip_finetuning\", sync_tensorboard=True, name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/vqa_project/baselines/tracking_information.pkl', 'rb') as f:\n",
    "    tracking = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss, eval_loss, lr = tracking[-1]\n",
    "eval_loss = min(i[1] for i in tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3.24e-05)#lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "bleu_scorers = [BLEUScore(n_gram=i) for i in [1, 2, 3]]\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "min_eval_loss = 0.18760925092543795*len(valid_dataloader) # float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "bl1 = []\n",
    "bl2 = []\n",
    "bl3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_masked = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    labels = 0\n",
    "    input_ids = 0\n",
    "    pixel_values = 0\n",
    "    attention_masked = 0\n",
    "    # with torch.no_grad():\n",
    "    for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "        wandb.log({\"val_loss\": eval_loss})\n",
    "    \n",
    "    real = processor.batch_decode(labels, skip_special_tokens=True)    \n",
    "    out = model.generate(input_ids, pixel_values, attention_masked)\n",
    "    pred = processor.batch_decode(out, skip_special_tokens=True) \n",
    "\n",
    "    bl1.append(bleu_scorers[0](pred, real))\n",
    "    bl2.append(bleu_scorers[1](pred, real))\n",
    "    bl3.append(bleu_scorers[2](pred, real))\n",
    "\n",
    "    wandb.log({\n",
    "        \"bleu_1\": mean([tensor.item() for tensor in bl1]),\n",
    "        \"bleu_2\": mean([tensor.item() for tensor in bl2]),\n",
    "        \"bleu_3\": mean([tensor.item() for tensor in bl3])\n",
    "    })\n",
    "    print(real[0], pred[0])\n",
    "    tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step()\n",
    "    if eval_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"/home/jovyan/vqa_project/baselines/finetuning/blip_vqa_base_tune\", from_pt=True) \n",
    "        print(\"/home/jovyan/vqa_project/baselines/finetuning/blip_vqa_base_tune\")\n",
    "        min_eval_loss = eval_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            break\n",
    "    \n",
    "    \n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "print(\"The finetuning process has done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/jovyan/vqa_project/baselines/saved_models/finetune_blip2/another\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "    input_ids = batch.pop('input_ids').to(device)\n",
    "    pixel_values = batch.pop('pixel_values').to(device)\n",
    "    attention_masked = batch.pop('attention_mask').to(device)\n",
    "    labels = batch.pop('labels').to(device)\n",
    "    \n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    # attention_mask=attention_masked,\n",
    "                    labels=labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jovyan/vqa_project/baselines/VQAv2_train_translation.jsonl\", 'r') as f:\n",
    "    infdataset = json.loads(list(f)[0])\n",
    "question = infdataset[1]['question']\n",
    "image_path = \"/home/jovyan/vqa_project/baselines/trainvqa/train2014/\" + infdataset[1]['image_id'] + \".jpg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
