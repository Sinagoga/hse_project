{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers\n",
    "# %pip install datasets\n",
    "# %pip install torch\n",
    "# %pip install torchmetrics\n",
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from torchmetrics.text import BLEUScore\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcc8b19ff50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"/home/jovyan/vqa_project/baselines/finetuning/blip_vqa_base_tune\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, processor, imagespath_split):\n",
    "        # self.dataset = dataset\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.dataset = json.loads(list(f)[0])\n",
    "        self.processor = processor\n",
    "        self.imagespath_split = imagespath_split\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get image + text\n",
    "        question = self.dataset[idx]['question']\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        if (\"val\" in self.imagespath_split):\n",
    "            image_path = self.imagespath_split + self.dataset[idx]['image_id'].replace(\"train\", \"val\") + \".jpg\"\n",
    "        else:\n",
    "            image_path = self.imagespath_split + self.dataset[idx]['image_id'] + \".jpg\"\n",
    "        # image_id = self.dataset[idx]['pid']\n",
    "        # image_path = f\"Data/train_fill_in_blank/{image_id}/image.png\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # image = image.resize((224, 224))\n",
    "        text = question\n",
    "        \n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\",  max_length=60)\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, padding='max_length', truncation=True, max_length=20, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        # labels = self.processor.tokenizer(answer, padding='max_length', truncation=True, max_length=8, return_tensors='pt')['input_ids']\n",
    "\n",
    "        encoding[\"labels\"] = labels\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():  encoding[k] = v.squeeze()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VQADataset(dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_train_translation.jsonl\",\n",
    "                          processor=processor,\n",
    "                          imagespath_split=\"/home/jovyan/vqa_project/baselines/trainvqa/train2014/\")\n",
    "valid_dataset = VQADataset(dataset_path=\"/home/jovyan/vqa_project/baselines/VQAv2_val_translation.jsonl\",\n",
    "                          processor=processor,\n",
    "                          imagespath_split=\"/home/jovyan/vqa_project/baselines/valvqa/val2014/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 36\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=20)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=24, shuffle=False, pin_memory=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrajanmihajlov\u001b[0m (\u001b[33maid_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/vqa_project/baselines/finetuning/wandb/run-20240530_134213-1n9pi6pe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aid_/blip_finetuning/runs/1n9pi6pe' target=\"_blank\">volcanic-wind-18</a></strong> to <a href='https://wandb.ai/aid_/blip_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aid_/blip_finetuning' target=\"_blank\">https://wandb.ai/aid_/blip_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aid_/blip_finetuning/runs/1n9pi6pe' target=\"_blank\">https://wandb.ai/aid_/blip_finetuning/runs/1n9pi6pe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aid_/blip_finetuning/runs/1n9pi6pe?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcd27b60250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"278590c2621521efe866317352d7f3e13fef885f\")\n",
    "wandb.init(project=\"blip_finetuning\", sync_tensorboard=True, name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/vqa_project/baselines/tracking_information.pkl', 'rb') as f:\n",
    "    tracking = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss, eval_loss, lr = tracking[-1]\n",
    "eval_loss = min(i[1] for i in tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3.24e-05)#lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "bleu_scorers = [BLEUScore(n_gram=i) for i in [1, 2, 3]]\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "min_eval_loss = 0.18760925092543795*len(valid_dataloader) # float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "bl1 = []\n",
    "bl2 = []\n",
    "bl3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...:   0%|          | 0/12327 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_masked = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    labels = 0\n",
    "    input_ids = 0\n",
    "    pixel_values = 0\n",
    "    attention_masked = 0\n",
    "    # with torch.no_grad():\n",
    "    for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "        wandb.log({\"val_loss\": eval_loss})\n",
    "    \n",
    "    real = processor.batch_decode(labels, skip_special_tokens=True)    \n",
    "    out = model.generate(input_ids, pixel_values, attention_masked)\n",
    "    pred = processor.batch_decode(out, skip_special_tokens=True) \n",
    "\n",
    "    bl1.append(bleu_scorers[0](pred, real))\n",
    "    bl2.append(bleu_scorers[1](pred, real))\n",
    "    bl3.append(bleu_scorers[2](pred, real))\n",
    "\n",
    "    wandb.log({\n",
    "        \"bleu_1\": mean([tensor.item() for tensor in bl1]),\n",
    "        \"bleu_2\": mean([tensor.item() for tensor in bl2]),\n",
    "        \"bleu_3\": mean([tensor.item() for tensor in bl3])\n",
    "    })\n",
    "    print(real[0], pred[0])\n",
    "    tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step()\n",
    "    if eval_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"/home/jovyan/vqa_project/baselines/finetuning/blip_vqa_base_tune\", from_pt=True) \n",
    "        print(\"/home/jovyan/vqa_project/baselines/finetuning/blip_vqa_base_tune\")\n",
    "        min_eval_loss = eval_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            break\n",
    "    \n",
    "    \n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "print(\"The finetuning process has done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/vqa_project/baselines/finetuning/blip_fintuning2.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bssh-sr002-jupyter.ai.cloud.ru/home/jovyan/vqa_project/baselines/finetuning/blip_fintuning2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msave_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/home/jovyan/vqa_project/baselines/saved_models/finetune_blip2/another\u001b[39;49m\u001b[39m\"\u001b[39;49m, from_pt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:2612\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2608\u001b[0m \u001b[39mfor\u001b[39;00m shard_file, shard \u001b[39min\u001b[39;00m shards\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   2609\u001b[0m     \u001b[39mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   2610\u001b[0m         \u001b[39m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   2611\u001b[0m         \u001b[39m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 2612\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mformat\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m   2613\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2614\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/safetensors/torch.py:284\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(\n\u001b[1;32m    254\u001b[0m     tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    255\u001b[0m     filename: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    256\u001b[0m     metadata: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m ):\n\u001b[1;32m    258\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[39m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/safetensors/torch.py:488\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m failing:\n\u001b[1;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    482\u001b[0m \u001b[39m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[39m{\u001b[39;00mfailing\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    489\u001b[0m     k: {\n\u001b[1;32m    490\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(v\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m    491\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: v\u001b[39m.\u001b[39mshape,\n\u001b[1;32m    492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: _tobytes(v, k),\n\u001b[1;32m    493\u001b[0m     }\n\u001b[1;32m    494\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    495\u001b[0m }\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/safetensors/torch.py:492\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m failing:\n\u001b[1;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    482\u001b[0m \u001b[39m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[39m{\u001b[39;00mfailing\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    488\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    489\u001b[0m     k: {\n\u001b[1;32m    490\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(v\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m    491\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: v\u001b[39m.\u001b[39mshape,\n\u001b[0;32m--> 492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: _tobytes(v, k),\n\u001b[1;32m    493\u001b[0m     }\n\u001b[1;32m    494\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    495\u001b[0m }\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/safetensors/torch.py:452\u001b[0m, in \u001b[0;36m_tobytes\u001b[0;34m(tensor, name)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[39m# Not in place as that would potentially modify a live running model\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mview(npdtype)\u001b[39m.\u001b[39mbyteswap(inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 452\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mtobytes()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"/home/jovyan/vqa_project/baselines/saved_models/finetune_blip2/another\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "    input_ids = batch.pop('input_ids').to(device)\n",
    "    pixel_values = batch.pop('pixel_values').to(device)\n",
    "    attention_masked = batch.pop('attention_mask').to(device)\n",
    "    labels = batch.pop('labels').to(device)\n",
    "    \n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    # attention_mask=attention_masked,\n",
    "                    labels=labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jovyan/vqa_project/baselines/VQAv2_train_translation.jsonl\", 'r') as f:\n",
    "    infdataset = json.loads(list(f)[0])\n",
    "question = infdataset[1]['question']\n",
    "image_path = \"/home/jovyan/vqa_project/baselines/trainvqa/train2014/\" + infdataset[1]['image_id'] + \".jpg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
