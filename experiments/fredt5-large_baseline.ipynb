{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-13T23:14:37.534352Z","iopub.status.busy":"2024-05-13T23:14:37.533955Z","iopub.status.idle":"2024-05-13T23:15:01.119580Z","shell.execute_reply":"2024-05-13T23:15:01.118546Z","shell.execute_reply.started":"2024-05-13T23:14:37.534318Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as nnf\n","from torch.amp import autocast\n","from torch import einsum\n","import torch.nn.functional as F\n","\n","import open_clip\n","\n","from transformers import GPT2LMHeadModel, AutoTokenizer\n","from transformers import T5ForConditionalGeneration\n","from typing import Optional\n","\n","from transformers.optimization import Adafactor\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","import pickle\n","from torchmetrics.text import BLEUScore\n","from evaluate import load\n","from statistics import mean\n","import pandas as pd\n","from einops import rearrange\n","import math\n","import wandb\n","from accelerate import Accelerator\n","from accelerate.utils import set_seed\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def exists(val):\n","    return val is not None\n","\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","\n","def stable_softmax(t, dim=-1):\n","    t = t - t.amax(dim=dim, keepdim=True)\n","    return t.softmax(dim=dim)\n","\n","\n","def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = nnf.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention\n","\n","\n","def expand_mask(mask):\n","    assert mask.ndim > 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n","    if mask.ndim == 3:\n","        mask = mask.unsqueeze(1)\n","    while mask.ndim < 4:\n","        mask = mask.unsqueeze(0)\n","    return mask\n","\n","\n","class BidirectionalCrossAttention(nn.Module):\n","    def __init__(\n","            self,\n","            *,\n","            dim,\n","            heads=8,\n","            dim_head=64,\n","            context_dim=None,\n","            dropout=0.,\n","            talking_heads=False,\n","            prenorm=False,\n","    ):\n","        super().__init__()\n","        context_dim = default(context_dim, dim)\n","        self.norm = nn.LayerNorm(dim) if prenorm else nn.Identity()\n","        self.context_norm = nn.LayerNorm(context_dim) if prenorm else nn.Identity()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        inner_dim = dim_head * heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.context_dropout = nn.Dropout(dropout)\n","        self.to_qk = nn.Linear(dim, inner_dim, bias=False)\n","        self.context_to_qk = nn.Linear(context_dim, inner_dim, bias=False)\n","        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n","        self.context_to_v = nn.Linear(context_dim, inner_dim, bias=False)\n","        self.to_out = nn.Linear(inner_dim, dim)\n","        self.context_to_out = nn.Linear(inner_dim, context_dim)\n","        self.talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else nn.Identity()\n","        self.context_talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else nn.Identity()\n","\n","    def forward(\n","            self,\n","            x,\n","            context,\n","            mask=None,\n","            context_mask=None,\n","            return_attn=False,\n","            rel_pos_bias=None\n","    ):\n","        b, i, j, h, device = x.shape[0], x.shape[-2], context.shape[-2], self.heads, x.device\n","        x = self.norm(x)\n","        context = self.context_norm(context)\n","        qk, v = self.to_qk(x), self.to_v(x)\n","        context_qk, context_v = self.context_to_qk(context), self.context_to_v(context)\n","        qk, context_qk, v, context_v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h),\n","                                           (qk, context_qk, v, context_v))\n","        sim = einsum('b h i d, b h j d -> b h i j', qk, context_qk) * self.scale\n","        if exists(rel_pos_bias):\n","            sim = sim + rel_pos_bias\n","        if exists(mask) or exists(context_mask):\n","            mask = default(mask, torch.ones((b, i), device=device, dtype=torch.bool))\n","            context_mask = default(context_mask, torch.ones((b, j), device=device, dtype=torch.bool))\n","            attn_mask = rearrange(mask, 'b i -> b 1 i 1') * rearrange(context_mask, 'b j -> b 1 1 j')\n","            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n","        attn = stable_softmax(sim, dim=-1)\n","        context_attn = stable_softmax(sim, dim=-2)\n","        attn = self.dropout(attn)\n","        context_attn = self.context_dropout(context_attn)\n","        attn = self.talking_heads(attn)\n","        context_attn = self.context_talking_heads(context_attn)\n","        out = einsum('b h i j, b h j d -> b h i d', attn, context_v)\n","        context_out = einsum('b h j i, b h j d -> b h i d', context_attn, v)\n","        out, context_out = map(lambda t: rearrange(t, 'b h n d -> b n (h d)'), (out, context_out))\n","        out = self.to_out(out)\n","        context_out = self.context_to_out(context_out)\n","        if return_attn:\n","            return out, context_out, attn, context_attn\n","        return out, context_out\n","\n","\n","class MultiheadAttention(nn.Module):\n","\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n","\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        nn.init.xavier_uniform_(self.qkv_proj.weight)\n","        self.qkv_proj.bias.data.fill_(0)\n","        nn.init.xavier_uniform_(self.o_proj.weight)\n","        self.o_proj.bias.data.fill_(0)\n","\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        if mask is not None:\n","            mask = expand_mask(mask)\n","        qkv = self.qkv_proj(x)\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3)\n","        q, k, v = qkv.chunk(3, dim=-1)\n","        values, attention = scaled_dot_product(q, k, v, mask=mask)\n","        values = values.permute(0, 2, 1, 3)\n","        values = values.reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o\n","\n","\n","class QFormerBlock(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, output_size, bias=True, act=nn.Tanh):\n","        super(QFormerBlock, self).__init__()\n","\n","        self.attn = MultiheadAttention(text_emb_size, text_emb_size, 16)\n","        self.cross_attn = BidirectionalCrossAttention(\n","            dim=img_emb_size,\n","            heads=16,\n","            dim_head=1024,\n","            context_dim=text_emb_size\n","        )\n","        self.text_mlp = nn.Sequential(\n","            nn.Linear(text_emb_size, text_emb_size * 2),\n","            act(),\n","            nn.Linear(text_emb_size * 2, text_emb_size * 2),\n","            act(),\n","            nn.Linear(text_emb_size * 2, output_size)\n","        )\n","\n","    @autocast(\"cuda\")\n","    def forward(self, img_emb: torch.Tensor, text_emb: torch.Tensor) -> torch.Tensor:\n","        text_emb = self.attn(text_emb)\n","        img_emb, text_emb = self.cross_attn(img_emb.reshape(-1, 1, img_emb.shape[1]), text_emb)\n","        text_emb = self.text_mlp(text_emb)\n","        return img_emb, text_emb\n","\n","\n","class MySequential(nn.Sequential):\n","    def forward(self, *inp):\n","        for module in self._modules.values():\n","            inp = module(*inp)\n","            if inp[0].shape[1] == 1:\n","                inp = (inp[0][:, 0, :], inp[1])\n","        return inp\n","\n","\n","class QFormer(nn.Module):\n","    def __init__(self, img_emb_size, text_emb_size, output_size, n_blocks=4, bias=True, act=nn.Tanh):\n","        super(QFormer, self).__init__()\n","\n","        self.blocks = MySequential(\n","            *[QFormerBlock(img_emb_size, text_emb_size, text_emb_size) for _ in range(n_blocks)],\n","        )\n","        self.res = nn.Sequential(\n","            nn.Linear(img_emb_size + text_emb_size, output_size)\n","        )\n","\n","    @autocast(\"cuda\")\n","    def forward(self, img_emb: torch.Tensor, text_emb: torch.Tensor) -> torch.Tensor:\n","        img_emb, text_emb = self.blocks(img_emb, text_emb)\n","        text_emb = text_emb.mean(axis=1)\n","        res_emb = torch.cat((img_emb, text_emb), axis=1)\n","        res_emb = self.res(res_emb)\n","        return res_emb\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_shape, output_shape, act=nn.Tanh):\n","        super(MLP, self).__init__()\n","        self.seq = nn.Sequential(\n","            nn.Linear(input_shape, input_shape * 2),\n","            act(),\n","            nn.Linear(input_shape * 2, output_shape)\n","        )\n","\n","    @autocast(\"cuda\")\n","    def forward(self, x):\n","        return self.seq(x)\n","\n","\n","def freeze(\n","        model,\n","        freeze_emb=False,\n","        freeze_ln=False,\n","        freeze_attn=False,\n","        freeze_ff=False,\n","        freeze_other=False,\n","):\n","    for name, p in model.named_parameters():\n","        name = name.lower()\n","        if 'ln' in name or 'norm' in name:\n","            p.requires_grad = not freeze_ln\n","        elif 'embeddings' in name:\n","            p.requires_grad = not freeze_emb\n","        elif 'mlp' in name:\n","            p.requires_grad = not freeze_ff\n","        elif 'attn' in name:\n","            p.requires_grad = not freeze_attn\n","        else:\n","            p.requires_grad = not freeze_other\n","\n","    return model\n","\n","\n","class ClipCaptionModel(nn.Module):\n","    def __init__(self, config, prefix_length: int, prefix_size: int = 640, dist_loss=nn.MSELoss()):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","        self.gpt = T5ForConditionalGeneration.from_pretrained(config.decoder,\n","                                                   eos_token_id=self.tokenizer.pad_token_id)\n","        self.gpt_embedding_size = self.gpt.get_input_embeddings().weight.shape[1]\n","        self.clip_project = QFormer(prefix_size, self.gpt_embedding_size,\n","                                    self.gpt_embedding_size * prefix_length)\n","        self.device = config.device\n","        self.dist_loss = dist_loss\n","        self.mlp = MLP(self.gpt_embedding_size, self.gpt_embedding_size)\n","\n","        for p in self.gpt.parameters():\n","            p.requires_grad = False\n","        for p in self.clip_model.parameters():\n","            p.requires_grad = False\n","\n","    def get_text_embeddings(self, tokens):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        return embedding_text\n","    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    @autocast(\"cuda\")\n","    def forward(self, query_tokens: torch.Tensor, query_mask: Optional[torch.Tensor],\n","                answer_tokens: torch.Tensor, answer_mask: Optional[torch.Tensor], image):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt(inputs_embeds=prefix_projections, labels=answer_tokens)\n","        return out, prefix_projections\n","\n","    def generate(self, image, texts, max_seq_len):\n","        tokens = torch.tensor(self.tokenizer.batch_encode_plus(texts, padding='max_length', max_length=max_seq_len, truncation=True)['input_ids'], dtype=torch.int64).to(self.device)\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt.generate(\n","            inputs_embeds=prefix_projections,\n","            max_new_tokens=self.prefix_length,\n","            no_repeat_ngram_size=3,\n","            repetition_penalty=2.,\n","        )\n","        res = [decode_question(x, self.tokenizer) for x in out]\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","import sys\n","from matplotlib import pyplot as plt\n","import json\n","from PIL import Image\n","class VQAv2_Dataset(Dataset):\n","    def __init__(self, config, dataset_path, coef_size=0.1,\n","                 tokenizer_name=\"\", prefix_length=20, normalize_prefix=False, imagespath_split=None):\n","        if not tokenizer_name:\n","            tokenizer_name = config.decoder\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        clip_model, _, self.preprocess = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.prefix_length = prefix_length\n","        self.normalize_prefix = normalize_prefix\n","\n","        with open(dataset_path, 'r') as f:\n","            dataset = json.loads(list(f)[0])\n","\n","        self.img_paths = []\n","        self.query_tokens = []\n","        self.answer_tokens = []\n","\n","        max_img = len(dataset)*coef_size\n","        for i, el in tqdm(enumerate(dataset), total=max_img):\n","            answer = el['answer'] \n","            question = el['question']\n","            self.query_tokens += [self.tokenizer.encode(question, return_tensors=\"pt\",padding='max_length', max_length=prefix_length, truncation=True)]\n","            self.answer_tokens += [self.tokenizer.encode(answer, return_tensors=\"pt\", padding='max_length', max_length=prefix_length, truncation=True)]\n","            if (\"val\" in imagespath_split):\n","                self.img_paths += [imagespath_split + el['image_id'].replace(\"train\", \"val\") + \".jpg\"]\n","            else:\n","                self.img_paths += [imagespath_split + el['image_id'] + \".jpg\"]\n","            if int(i) >= max_img:\n","                  break\n","        del dataset\n","        sys.stdout.flush()\n","        self.max_seq_len = prefix_length\n","\n","    def pad_tokens(self, item: int):\n","        query_tokens = self.query_tokens[item]\n","        answer_tokens = self.answer_tokens[item]\n","        query_mask = query_tokens\n","        answer_mask = answer_tokens\n","        return query_tokens[0], query_mask[0], answer_tokens[0], answer_mask[0]\n","\n","    def get_image(self, item):\n","        name = str(self.img_paths[item])\n","        image_resized = Image.open(name)\n","        image_resized = image_resized.resize((256, 256))\n","        return image_resized\n","\n","    def __len__(self) -> int:\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, item):\n","        image = self.get_image(item)\n","        image = self.preprocess(image).unsqueeze(0)\n","        query_tokens, query_mask, answer_tokens, answer_mask = self.pad_tokens(item)\n","        return query_tokens, query_mask, answer_tokens, answer_mask, image[0], item\n","\n","    def show_image(self, item):\n","        image = self.get_image(item)\n","        text = self.tokenizer.decode(self.pad_tokens(item)[2])\n","        plt.imshow(image)\n","        print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Config:\n","    encoder: str = \"ViT-B-16-plus-240\"\n","    decoder: str = \"ai-forever/FRED-T5-large\"\n","    batch_size: int = 128\n","    num_epochs: int = 40\n","    frozen_gpt: int = 8\n","    frozen_clip: int = 24\n","    learning_rate: float  = 2e-4\n","    save_path: str = \"saved_models_FRED-T5-large/fulliliya/\"\n","    prefix_length: int = 20\n","    only_prefix: int = False\n","    prefix: str = \"prefix_small\"\n","    device: str = \"cuda:1\"\n","    save_every: int = 1\n","    warmup_steps: int = 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:10.238419Z","iopub.status.busy":"2024-05-12T16:35:10.237459Z","iopub.status.idle":"2024-05-12T16:35:12.132381Z","shell.execute_reply":"2024-05-12T16:35:12.131436Z","shell.execute_reply.started":"2024-05-12T16:35:10.238377Z"},"trusted":true},"outputs":[],"source":["bertscore = load(\"bertscore\")\n","meteor = load('meteor')\n","rouge = load('rouge')\n","bleu_scorers = [BLEUScore(n_gram=i) for i in [1, 2, 3]] + [bertscore, meteor, rouge]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:14.372918Z","iopub.status.busy":"2024-05-12T16:35:14.372562Z","iopub.status.idle":"2024-05-12T16:35:46.471596Z","shell.execute_reply":"2024-05-12T16:35:46.470408Z","shell.execute_reply.started":"2024-05-12T16:35:14.372887Z"},"trusted":true},"outputs":[],"source":["wandb.login(key=\"\")\n","wandb.init(project=\"baseline-vqa-rugpt\", sync_tensorboard=True, name=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:17:48.923946Z","iopub.status.busy":"2024-05-13T23:17:48.923232Z","iopub.status.idle":"2024-05-13T23:17:48.929456Z","shell.execute_reply":"2024-05-13T23:17:48.928365Z","shell.execute_reply.started":"2024-05-13T23:17:48.923911Z"},"trusted":true},"outputs":[],"source":["def decode_question(question_token, tokenizer):\n","    decoded_string = tokenizer.decode(question_token)\n","    # if \"<pad>\" in decoded_string:\n","    #     truncate_pads = decoded_string.index(\"<pad>\")\n","    #     decoded_string = decoded_string[:truncate_pads]\n","    decoded_string = decoded_string.replace(\"<pad>\", \"\")\n","    return decoded_string"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:17:52.010474Z","iopub.status.busy":"2024-05-13T23:17:52.009843Z","iopub.status.idle":"2024-05-13T23:17:52.021926Z","shell.execute_reply":"2024-05-13T23:17:52.020878Z","shell.execute_reply.started":"2024-05-13T23:17:52.010442Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def train(model, optimizer, scheduler, loss_func, loader, epoch, args):\n","    model.train()\n","    pbar = tqdm(loader, total=len(loader))\n","    step = 0\n","    for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","\n","        query_tokens, query_mask, prefix = query_tokens.to(args.device), query_mask.to(args.device), prefix.to(\n","            args.device, dtype=torch.bfloat16)\n","        answer_tokens, answer_mask = answer_tokens.to(args.device), answer_mask.to(args.device)\n","        # print(query_tokens.size(), query_mask.size(), answer_tokens.size(), answer_mask.size(), prefix.size())\n","        outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","        logits = outputs.logits\n","        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), answer_tokens.flatten().to(torch.int64),\n","                                 ignore_index=0)\n","\n","        loss2 = model.dist_loss(model.get_text_embeddings(answer_tokens).to(torch.float32), proj.to(torch.float32))\n","        loss += loss2\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n","\n","        #backpropogation\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        pbar.set_postfix({\"loss\": loss.item(), \"dist_loss\": loss2.item()})\n","        wandb.log({\"loss\": loss.item(), \"dist_loss\": loss2.item()})\n","        step += 1\n","        if step % 1000 == 0:\n","            print(\"QUESTION:\", decode_question(query_tokens[0], train_dataset.tokenizer))\n","            print(\"ANSWER:\", decode_question(answer_tokens[0], train_dataset.tokenizer))\n","            print(\"PREDICTED: \", model.generate(torch.tensor([train_dataset[idx[0]][4].tolist()]).to(args.device),\n","                                                [decode_question(query_tokens[0], model.tokenizer)], train_dataset.max_seq_len)[0])\n","    with open(f'{args.save_path}checkpoint_{epoch}.pkl', 'wb') as f:\n","        pickle.dump(model, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:35:46.499711Z","iopub.status.busy":"2024-05-12T16:35:46.499126Z","iopub.status.idle":"2024-05-12T16:35:46.521515Z","shell.execute_reply":"2024-05-12T16:35:46.520544Z","shell.execute_reply.started":"2024-05-12T16:35:46.499679Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, optimizer, scheduler, loss_func, loader, args):\n","    model.eval()\n","    pbar = tqdm(loader, total=len(loader))\n","    step = 0\n","\n","    bl1 = []\n","    bl2 = []\n","    bl3 = []\n","    brt = []\n","    mtr = []\n","    rg = []\n","    val_losses = []\n","    val_dist = []\n","    for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","        query_tokens, query_mask, prefix = query_tokens.to(args.device), query_mask.to(args.device), prefix.to(\n","            args.device, dtype=torch.bfloat16)\n","        answer_tokens, answer_mask = answer_tokens.to(args.device), answer_mask.to(args.device)\n","        outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)\n","        logits = outputs.logits\n","        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), answer_tokens.flatten().to(torch.int64),\n","                                 ignore_index=0)\n","        loss2 = model.dist_loss(model.get_text_embeddings(answer_tokens), proj)\n","\n","        # real = model.tokenizer.batch_decode(answer_tokens)\n","        real = [decode_question(answer_tokens[i], model.tokenizer) for i in range(len(answer_tokens))]\n","#         pred = model.generate(torch.tensor([val_dataset[idx[j]][4].tolist() for j in range(len(idx))]).to(args.device),\n","#                               [\"Что на картинке?\" for _ in range(len(idx))])\n","        pred = model.generate(torch.tensor([val_dataset[idx[j]][4].tolist() for j in range(len(idx))]).to(args.device),\n","                              [decode_question(query_tokens[j], model.tokenizer) for j in range(len(idx))], val_dataset.max_seq_len)\n","        \n","#         model.generate(torch.tensor([train_dataset[idx[0]][4].tolist()]).to(args.device),\n","#                                                 decode_question(query_tokens, model.tokenizer))[0]\n","        \n","        # real = truncate_sentences(real)\n","        # pred = truncate_sentences(pred)\n","        \n","        bl1.append(bleu_scorers[0](pred, real))\n","        bl2.append(bleu_scorers[1](pred, real))\n","        bl3.append(bleu_scorers[2](pred, real))\n","        brt.append(bleu_scorers[3].compute(predictions=pred, references=real, lang=\"ru\")['f1'])\n","        mtr.append(bleu_scorers[4].compute(predictions=pred, references=real)['meteor'])\n","        rg.append(bleu_scorers[5].compute(predictions=pred, references=real)['rougeL'])\n","\n","        if step % 400 == 0:\n","            print(\"QUESTION:\", decode_question(query_tokens[0], val_dataset.tokenizer))\n","            print(\"TEXT:\", real[0])\n","            print(\"PREDICTED: \", pred[0])\n","\n","            imgs = []\n","            for j in range(len(idx)):\n","                wa_img = wandb.Image(\n","                    val_dataset.get_image(idx[j]),\n","                    caption=f\"REAL : {real[j]}, PREDICTED : {pred[j]}\"\n","                )\n","                imgs.append(wa_img)\n","\n","            wandb.log({\"Generations.\": imgs})\n","\n","        step += 1\n","\n","        pbar.set_postfix({\"val_loss\": loss.item(), \"val_dist\": loss2.item()})\n","        val_losses.append(loss.item())\n","        val_dist.append(loss2.item())\n","\n","    wandb.log({\"val_loss\": mean(val_losses),\n","               \"val_dist\": mean(val_dist)})\n","\n","    wandb.log({\n","        \"bleu_1\": mean([tensor.item() for tensor in bl1]),\n","        \"bleu_2\": mean([tensor.item() for tensor in bl2]),\n","        \"bleu_3\": mean([tensor.item() for tensor in bl3]),\n","        \"bert_score\": np.mean([tensor for tensor in brt]),\n","        \"meteor_score\": np.mean([tensor for tensor in mtr]),\n","        \"rouge_score\": np.mean([tensor for tensor in rg])\n","    })\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:36:04.797205Z","iopub.status.busy":"2024-05-12T16:36:04.796511Z","iopub.status.idle":"2024-05-12T16:36:04.809456Z","shell.execute_reply":"2024-05-12T16:36:04.808123Z","shell.execute_reply.started":"2024-05-12T16:36:04.797173Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def fit_model(args, model, train_loader, val_loader):\n","    wandb.config = {\n","        \"learning_rate\": args.learning_rate,\n","        \"epochs\": args.num_epochs,\n","        \"batch_size\": args.batch_size\n","    }\n","\n","    # if not os.path.exists(args.save_path):\n","    #     os.makedirs(args.save_path)\n","    device = args.device\n","\n","    # model = ClipCaptionModel(args, args.prefix_length)\n","    model = model.to(args.device)\n","\n","    wandb.watch(model, log_freq=10, log=\"gradients\")\n","\n","    model.train()\n","\n","    loss_func = nn.CrossEntropyLoss()\n","    optimizer = Adafactor(model.parameters(), lr=args.learning_rate,\n","                          relative_step=False  # for adafactor\n","                          )\n","\n","    # train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","    # val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=15000\n","    )\n","    # evaluate(model, optimizer, scheduler, loss_func, val_loader, args)\n","    print(\"Start train model\")\n","    for epoch in range(args.num_epochs):\n","        if epoch == args.frozen_gpt:\n","            print(\"GPT UNFROZEN\")\n","            for p in model.gpt.parameters():\n","                p.requires_grad = True\n","        if epoch == args.frozen_clip:\n","            print(\"CLIP UNFROZEN\")\n","            for p in model.clip_model.parameters():\n","                p.requires_grad = True\n","        print(f\"---------- Train epoch {epoch} ---------\")\n","        train(model, optimizer, scheduler, loss_func, train_loader, epoch, args)\n","        print(f\"---------- Evaluate epoch {epoch} ---------\")\n","        evaluate(model, optimizer, scheduler, loss_func, val_loader, args)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:18:13.833872Z","iopub.status.busy":"2024-05-13T23:18:13.833039Z","iopub.status.idle":"2024-05-13T23:18:22.979012Z","shell.execute_reply":"2024-05-13T23:18:22.978052Z","shell.execute_reply.started":"2024-05-13T23:18:13.833841Z"},"trusted":true},"outputs":[],"source":["config = Config()\n","train_dataset = VQAv2_Dataset(config, dataset_path=\"VQAv2_train_translation.jsonl\", imagespath_split=\"trainvqa/train2014/\", coef_size=0.5)\n","val_dataset = VQAv2_Dataset(config, dataset_path=\"VQAv2_val_translation.jsonl\", imagespath_split=\"valvqa/val2014/\", coef_size=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T23:53:37.689100Z","iopub.status.busy":"2024-05-13T23:53:37.688719Z","iopub.status.idle":"2024-05-13T23:53:47.886538Z","shell.execute_reply":"2024-05-13T23:53:47.885546Z","shell.execute_reply.started":"2024-05-13T23:53:37.689075Z"},"trusted":true},"outputs":[],"source":["model = ClipCaptionModel(config, config.prefix_length)\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)\n","val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=20, shuffle=True, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_model(config, model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embtext = []\n","imgemb = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ClipCaptionModel(nn.Module):\n","    def __init__(self, config, prefix_length: int, prefix_size: int = 640, dist_loss=nn.MSELoss()):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","        self.gpt = T5ForConditionalGeneration.from_pretrained(config.decoder)\n","                                                #    eos_token_id=self.tokenizer.pad_token_id)\n","        self.gpt_embedding_size = self.gpt.get_input_embeddings().weight.shape[1]\n","        self.clip_project = QFormer(prefix_size, self.gpt_embedding_size,\n","                                    self.gpt_embedding_size * prefix_length)\n","        self.device = config.device\n","        self.dist_loss = dist_loss\n","        self.mlp = MLP(self.gpt_embedding_size, self.gpt_embedding_size)\n","\n","        for p in self.gpt.parameters():\n","            p.requires_grad = False\n","        for p in self.clip_model.parameters():\n","            p.requires_grad = False\n","\n","    def get_text_embeddings(self, tokens):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        return embedding_text\n","    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    @autocast(\"cuda\")\n","    def forward(self, query_tokens: torch.Tensor, query_mask: Optional[torch.Tensor],\n","                answer_tokens: torch.Tensor, answer_mask: Optional[torch.Tensor], image):\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        embtext.append(embedding_text)\n","        imgemb.append(image)\n","        # print(image, embedding_text)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt(inputs_embeds=prefix_projections, labels=answer_tokens)\n","        return out, prefix_projections\n","\n","    def generate(self, image, texts, max_seq_len):\n","        tokens = torch.tensor(self.tokenizer.batch_encode_plus(texts, padding='max_length', max_length=max_seq_len, truncation=True)['input_ids'], dtype=torch.int64).to(self.device)\n","        with torch.no_grad():\n","            embedding_text = self.gpt.encoder.forward(input_ids=tokens, return_dict=True)\n","            embedding_text = embedding_text.last_hidden_state\n","        image = self.clip_model.encode_image(image)\n","        prefix_projections = self.clip_project(image.float(), embedding_text).view(-1, self.prefix_length,\n","                                                                                   self.gpt_embedding_size)\n","        prefix_projections = self.mlp(prefix_projections)\n","        out = self.gpt.generate(\n","            inputs_embeds=prefix_projections,\n","            max_new_tokens=self.prefix_length,\n","            no_repeat_ngram_size=3,\n","            repetition_penalty=2.,\n","        )\n","        res = [decode_question(x, self.tokenizer) for x in out]\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prefix_length = 20\n","clip_model, _, _ = open_clip.create_model_and_transforms(config.encoder, pretrained=\"laion400m_e32\")\n","tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","gpt = T5ForConditionalGeneration.from_pretrained(config.decoder,\n","                                            eos_token_id=tokenizer.pad_token_id)\n","gpt_embedding_size = gpt.get_input_embeddings().weight.shape[1]\n","clip_project = QFormer(512, gpt_embedding_size,\n","                            gpt_embedding_size * prefix_length)\n","device = config.device\n","dist_loss=nn.MSELoss()\n","mlp = MLP(gpt_embedding_size, gpt_embedding_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = ClipCaptionModel(config, 20)\n","model = model.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pbar = tqdm(train_loader, total=len(train_loader))\n","query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx = (0, 0, 0, 0, 0, 0)\n","for (query_tokens, query_mask, answer_tokens, answer_mask, prefix, idx) in pbar:\n","    query_tokens, query_mask, prefix = query_tokens.to(config.device), query_mask.to(config.device), prefix.to(\n","        config.device, dtype=torch.bfloat16)\n","    answer_tokens, answer_mask = answer_tokens.to(config.device), answer_mask.to(config.device)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query_tokens = query_tokens.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(config.decoder)\n","gpt = T5ForConditionalGeneration.from_pretrained(config.decoder,\n","                                                   eos_token_id=tokenizer.pad_token_id)\n","gpt = gpt.to(config.device)\n","with torch.no_grad():\n","    embedding_text = gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","    embedding_text = embedding_text.last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query_tokens[51]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["proj"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lm_text='<LM>Принялся Кутузов рассказывать свою историю как он сюда попал. Началось'\n","input_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\n","outputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\n","print(tokenizer.decode(outputs[0][1:]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gpt = gpt.to(config.device)\n","outputs = gpt.encoder.forward(input_ids=query_tokens, return_dict=True)\n","embeddings = outputs.last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = ClipCaptionModel(config, config.prefix_length)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = model.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs, proj = model(query_tokens, query_mask, answer_tokens, answer_mask, prefix)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4978927,"sourceId":8374280,"sourceType":"datasetVersion"},{"datasetId":4982204,"sourceId":8378551,"sourceType":"datasetVersion"},{"datasetId":4982248,"sourceId":8378604,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
